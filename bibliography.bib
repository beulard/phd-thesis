
@article{butter_how_2019,
	title = {How to {GAN} {LHC} Events},
	volume = {7},
	issn = {2542-4653},
	url = {http://arxiv.org/abs/1907.03764},
	doi = {10.21468/SciPostPhys.7.6.075},
	abstract = {Event generation for the {LHC} can be supplemented by generative adversarial networks, which generate physical events and avoid highly ineﬃcient event unweighting. For top pair production we show how such a network describes intermediate on-shell particles, phase space boundaries, and tails of distributions. In particular, we introduce the maximum mean discrepancy to resolve sharp local features. It can be extended in a straightforward manner to include for instance oﬀ-shell contributions, higher orders, or approximate detector effects.},
	pages = {075},
	number = {6},
	journaltitle = {{SciPost} Physics},
	shortjournal = {{SciPost} Phys.},
	author = {Butter, Anja and Plehn, Tilman and Winterhalder, Ramon},
	urldate = {2021-01-12},
	date = {2019-12-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.03764},
	keywords = {High Energy Physics - Phenomenology},
	file = {Butter et al. - 2019 - How to GAN LHC Events.pdf:/home/md618/Zotero/storage/ESDDXS8J/Butter et al. - 2019 - How to GAN LHC Events.pdf:application/pdf},
}

@article{matchev_uncertainties_2020,
	title = {Uncertainties associated with {GAN}-generated datasets in high energy physics},
	url = {http://arxiv.org/abs/2002.06307},
	abstract = {Recently, Generative Adversarial Networks ({GANs}) trained on samples of traditionally simulated collider events have been proposed as a way of generating larger simulated datasets at a reduced computational cost. In this paper we present an argument cautioning against the usage of this method to meet the simulation requirements of an experiment, namely that data generated by a {GAN} cannot statistically be better than the data it was trained on.},
	journaltitle = {{arXiv}:2002.06307 [hep-ex, physics:hep-ph, physics:physics]},
	author = {Matchev, Konstantin T. and Shyamsundar, Prasanth},
	urldate = {2021-01-12},
	date = {2020-05-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.06307},
	keywords = {High Energy Physics - Phenomenology, High Energy Physics - Experiment, Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability},
	file = {Matchev and Shyamsundar - 2020 - Uncertainties associated with GAN-generated datase.pdf:/home/md618/Zotero/storage/UNZ83PZS/Matchev and Shyamsundar - 2020 - Uncertainties associated with GAN-generated datase.pdf:application/pdf},
}
@inproceedings{NIPS2017_892c3b1c,
 author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Training of Wasserstein GANs},
 url = {https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf},
 volume = {30},
 year = {2017}
}



@article{karras_progressive_2018,
	title = {Progressive Growing of {GANs} for Improved Quality, Stability, and Variation},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly ﬁne details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., {CELEBA} images at 10242. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised {CIFAR}10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating {GAN} results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the {CELEBA} dataset.},
	journaltitle = {{arXiv}:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	urldate = {2021-01-12},
	date = {2018-02-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1710.10196},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Karras et al. - 2018 - Progressive Growing of GANs for Improved Quality, .pdf:/home/md618/Zotero/storage/YX5JFKQD/Karras et al. - 2018 - Progressive Growing of GANs for Improved Quality, .pdf:application/pdf},
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2021-01-12},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/RBTA6XG5/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/XLLX6VM7/1512.html:text/html},
}

@article{qin_how_2020,
	title = {How does Lipschitz Regularization Influence {GAN} Training?},
	url = {http://arxiv.org/abs/1811.09567},
	abstract = {Despite the success of Lipschitz regularization in stabilizing {GAN} training, the exact reason of its effectiveness remains poorly understood. The direct effect of \$K\$-Lipschitz regularization is to restrict the \$L2\$-norm of the neural network gradient to be smaller than a threshold \$K\$ (e.g., \$K=1\$) such that \${\textbackslash}{\textbar}{\textbackslash}nabla f{\textbackslash}{\textbar} {\textbackslash}leq K\$. In this work, we uncover an even more important effect of Lipschitz regularization by examining its impact on the loss function: It degenerates {GAN} loss functions to almost linear ones by restricting their domain and interval of attainable gradient values. Our analysis shows that loss functions are only successful if they are degenerated to almost linear ones. We also show that loss functions perform poorly if they are not degenerated and that a wide range of functions can be used as loss function as long as they are sufficiently degenerated by regularization. Basically, Lipschitz regularization ensures that all loss functions effectively work in the same way. Empirically, we verify our proposition on the {MNIST}, {CIFAR}10 and {CelebA} datasets.},
	journaltitle = {{arXiv}:1811.09567 [cs]},
	author = {Qin, Yipeng and Mitra, Niloy and Wonka, Peter},
	urldate = {2021-01-12},
	date = {2020-08-25},
	eprinttype = {arxiv},
	eprint = {1811.09567},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/QXNNRHEK/Qin et al. - 2020 - How does Lipschitz Regularization Influence GAN Tr.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/IG5Y3TYK/1811.html:text/html},
}

@article{xu_modeling_2019,
	title = {Modeling Tabular data using Conditional {GAN}},
	url = {http://arxiv.org/abs/1907.00503},
	abstract = {Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design {TGAN}, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. {TGAN} outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.},
	journaltitle = {{arXiv}:1907.00503 [cs, stat]},
	author = {Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
	urldate = {2021-01-12},
	date = {2019-10-27},
	eprinttype = {arxiv},
	eprint = {1907.00503},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/BBJVRSKM/Xu et al. - 2019 - Modeling Tabular data using Conditional GAN.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/3Y9BLDV2/1907.html:text/html},
}

@article{he_delving_2015,
	title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on {ImageNet} Classification},
	url = {http://arxiv.org/abs/1502.01852},
	shorttitle = {Delving Deep into Rectifiers},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit ({PReLU}) that generalizes the traditional rectified unit. {PReLU} improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our {PReLU} networks ({PReLU}-nets), we achieve 4.94\% top-5 test error on the {ImageNet} 2012 classification dataset. This is a 26\% relative improvement over the {ILSVRC} 2014 winner ({GoogLeNet}, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	journaltitle = {{arXiv}:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2021-01-12},
	date = {2015-02-06},
	eprinttype = {arxiv},
	eprint = {1502.01852},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/DIZJ6EBQ/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/ERSNSTRF/1502.html:text/html},
}

@article{fedus_many_2018,
	title = {Many Paths to Equilibrium: {GANs} Do Not Need to Decrease a Divergence At Every Step},
	url = {http://arxiv.org/abs/1710.08446},
	shorttitle = {Many Paths to Equilibrium},
	abstract = {Generative adversarial networks ({GANs}) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. {GANs} are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of {GANs} is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During {GAN} training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of {GAN} training as divergence minimization. Specifically, we demonstrate that {GANs} are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that {GAN} training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.},
	journaltitle = {{arXiv}:1710.08446 [cs, stat]},
	author = {Fedus, William and Rosca, Mihaela and Lakshminarayanan, Balaji and Dai, Andrew M. and Mohamed, Shakir and Goodfellow, Ian},
	urldate = {2021-01-12},
	date = {2018-02-20},
	eprinttype = {arxiv},
	eprint = {1710.08446},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/CG3ZR985/Fedus et al. - 2018 - Many Paths to Equilibrium GANs Do Not Need to Dec.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/AUT3VS2T/1710.html:text/html},
}

@article{karras_style-based_2019,
	title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	journaltitle = {{arXiv}:1812.04948 [cs, stat]},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	urldate = {2021-01-12},
	date = {2019-03-29},
	eprinttype = {arxiv},
	eprint = {1812.04948},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/MWT89ETG/Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/CEJIG2E5/1812.html:text/html},
}

@article{salimans_improved_2016,
	title = {Improved Techniques for Training {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks ({GANs}) framework. We focus on two applications of {GANs}: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on {MNIST}, {CIFAR}-10 and {SVHN}. The generated images are of high quality as confirmed by a visual Turing test: our model generates {MNIST} samples that humans cannot distinguish from real data, and {CIFAR}-10 samples that yield a human error rate of 21.3\%. We also present {ImageNet} samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of {ImageNet} classes.},
	journaltitle = {{arXiv}:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	urldate = {2021-01-12},
	date = {2016-06-10},
	eprinttype = {arxiv},
	eprint = {1606.03498},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/4LSFHM6R/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/PVPYXSCB/1606.html:text/html},
}

@online{sahoo_residual_2018,
	title = {Residual blocks — Building blocks of {ResNet}},
	url = {https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec},
	abstract = {Understanding  a residual block is quite easy. In traditional neural networks, each layer feeds into the next layer. In a network with…},
	titleaddon = {Medium},
	author = {Sahoo, Sabyasachi},
	urldate = {2021-01-12},
	date = {2018-11-29},
	langid = {english},
	file = {Snapshot:/home/md618/Zotero/storage/94FTKW7Q/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec.html:text/html},
}

@article{camino_generating_2018,
	title = {Generating Multi-Categorical Samples with Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1807.01202},
	abstract = {We propose a method to train generative adversarial networks on mutivariate feature vectors representing multiple categorical values. In contrast to the continuous domain, where {GAN}-based methods have delivered considerable results, {GANs} struggle to perform equally well on discrete data. We propose and compare several architectures based on multiple (Gumbel) softmax output layers taking into account the structure of the data. We evaluate the performance of our architecture on datasets with different sparsity, number of features, ranges of categorical values, and dependencies among the features. Our proposed architecture and method outperforms existing models.},
	journaltitle = {{arXiv}:1807.01202 [cs, stat]},
	author = {Camino, Ramiro and Hammerschmidt, Christian and State, Radu},
	urldate = {2021-01-12},
	date = {2018-07-04},
	eprinttype = {arxiv},
	eprint = {1807.01202},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/TVBJ6R94/Camino et al. - 2018 - Generating Multi-Categorical Samples with Generati.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/4SYPH4LA/1807.html:text/html},
}

@online{bai_comprehensive_2019,
	title = {A Comprehensive Introduction to Different Types of Convolutions in Deep Learning},
	url = {https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215},
	abstract = {Towards intuitive understanding of convolutions through visualizations},
	titleaddon = {Medium},
	author = {Bai, Kunlun},
	urldate = {2021-01-12},
	date = {2019-02-11},
	langid = {english},
}

@article{miyato_spectral_2018,
	title = {Spectral Normalization for Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1802.05957},
	abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on {CIFAR}10, {STL}-10, and {ILSVRC}2012 dataset, and we experimentally confirmed that spectrally normalized {GANs} ({SN}-{GANs}) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
	journaltitle = {{arXiv}:1802.05957 [cs, stat]},
	author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	urldate = {2021-01-12},
	date = {2018-02-16},
	eprinttype = {arxiv},
	eprint = {1802.05957},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/IR7ZBE7F/Miyato et al. - 2018 - Spectral Normalization for Generative Adversarial .pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/YTUJFK3I/1802.html:text/html},
}

@inproceedings{wang_multiscale_2003,
	location = {Pacific Grove, {CA}, {USA}},
	title = {Multiscale structural similarity for image quality assessment},
	isbn = {978-0-7803-8104-9},
	url = {http://ieeexplore.ieee.org/document/1292216/},
	doi = {10.1109/ACSSC.2003.1292216},
	abstract = {The structural similarity image quality paradigm is based on the assumption that the human visual system is highly adapted for extracting structural information from the scene, and therefore a measure of structural similarity can provide a good approximation to perceived image quality. This paper proposes a multi-scale structural similarity method, which supplies more ﬂexibility than previous single-scale methods in incorporating the variations of viewing conditions. We develop an image synthesis method to calibrate the parameters that deﬁne the relative importance of different scales. Experimental comparisons demonstrate the effectiveness of the proposed method.},
	eventtitle = {Conference Record of the 37th Asilomar Conference on Signals, Systems and Computers},
	pages = {1398--1402},
	booktitle = {The Thrity-Seventh Asilomar Conference on Signals, Systems \& Computers, 2003},
	publisher = {{IEEE}},
	author = {Wang, Z. and Simoncelli, E.P. and Bovik, A.C.},
	urldate = {2021-01-12},
	date = {2003},
	langid = {english},
	file = {Wang et al. - 2003 - Multiscale structural similarity for image quality.pdf:/home/md618/Zotero/storage/QILE2AAZ/Wang et al. - 2003 - Multiscale structural similarity for image quality.pdf:application/pdf},
}

@article{barua_fcc-gan_2019,
	title = {{FCC}-{GAN}: A Fully Connected and Convolutional Net Architecture for {GANs}},
	url = {http://arxiv.org/abs/1905.02417},
	shorttitle = {{FCC}-{GAN}},
	abstract = {Generative Adversarial Networks ({GANs}) are a powerful class of generative models. Despite their successes, the most appropriate choice of a {GAN} network architecture is still not well understood. {GAN} models for image synthesis have adopted a deep convolutional network architecture, which eliminates or minimizes the use of fully connected and pooling layers in favor of convolution layers in the generator and discriminator of {GANs}. In this paper, we demonstrate that a convolution network architecture utilizing deep fully connected layers and pooling layers can be more effective than the traditional convolution-only architecture, and we propose {FCC}-{GAN}, a fully connected and convolutional {GAN} architecture. Models based on our {FCC}-{GAN} architecture learn both faster than the conventional architecture and also generate higher quality of samples. We demonstrate the effectiveness and stability of our approach across four popular image datasets.},
	journaltitle = {{arXiv}:1905.02417 [cs, stat]},
	author = {Barua, Sukarna and Erfani, Sarah Monazam and Bailey, James},
	urldate = {2021-01-12},
	date = {2019-05-27},
	eprinttype = {arxiv},
	eprint = {1905.02417},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/FDLP27WC/Barua et al. - 2019 - FCC-GAN A Fully Connected and Convolutional Net A.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/BC2J6GF4/1905.html:text/html},
}

@online{noauthor_towards_nodate,
	title = {Towards Data Set Augmentation With {GANs}},
	url = {https://www.jungle.ai/post/towards-data-set-augmentation-with-gans},
	urldate = {2021-01-12},
}

@online{noauthor_rmachinelearning_nodate,
	title = {r/{MachineLearning} - [D] Why do we use gumbel softmax instead of simple softmax for discrete cases?},
	url = {https://www.reddit.com/r/MachineLearning/comments/7fowm7/d_why_do_we_use_gumbel_softmax_instead_of_simple/},
	abstract = {17 votes and 9 comments so far on Reddit},
	titleaddon = {reddit},
	urldate = {2021-01-12},
	langid = {american},
	file = {Snapshot:/home/md618/Zotero/storage/ZNU8XBDJ/d_why_do_we_use_gumbel_softmax_instead_of_simple.html:text/html},
}

@article{paganini_calogan_2018,
	title = {{CaloGAN}: Simulating 3D High Energy Particle Showers in Multi-Layer Electromagnetic Calorimeters with Generative Adversarial Networks},
	volume = {97},
	issn = {2470-0010, 2470-0029},
	url = {http://arxiv.org/abs/1712.10321},
	doi = {10.1103/PhysRevD.97.014021},
	shorttitle = {{CaloGAN}},
	abstract = {The precise modeling of subatomic particle interactions and propagation through matter is paramount for the advancement of nuclear and particle physics searches and precision measurements. The most computationally expensive step in the simulation pipeline of a typical experiment at the Large Hadron Collider ({LHC}) is the detailed modeling of the full complexity of physics processes that govern the motion and evolution of particle showers inside calorimeters. We introduce {\textbackslash}textsc\{{CaloGAN}\}, a new fast simulation technique based on generative adversarial networks ({GANs}). We apply these neural networks to the modeling of electromagnetic showers in a longitudinally segmented calorimeter, and achieve speedup factors comparable to or better than existing full simulation techniques on {CPU} (\$100{\textbackslash}times\$-\$1000{\textbackslash}times\$) and even faster on {GPU} (up to \${\textbackslash}sim10{\textasciicircum}5{\textbackslash}times\$). There are still challenges for achieving precision across the entire phase space, but our solution can reproduce a variety of geometric shower shape properties of photons, positrons and charged pions. This represents a significant stepping stone toward a full neural network-based detector simulation that could save significant computing time and enable many analyses now and in the future.},
	pages = {014021},
	number = {1},
	journaltitle = {Physical Review D},
	shortjournal = {Phys. Rev. D},
	author = {Paganini, Michela and de Oliveira, Luke and Nachman, Benjamin},
	urldate = {2021-01-13},
	date = {2018-01-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.10321},
	keywords = {High Energy Physics - Phenomenology, High Energy Physics - Experiment, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Paganini et al. - 2018 - CaloGAN Simulating 3D High Energy Particle Shower.pdf:/home/md618/Zotero/storage/XEJQVCNE/Paganini et al. - 2018 - CaloGAN Simulating 3D High Energy Particle Shower.pdf:application/pdf},
}

@article{erdmann_precise_2019,
	title = {Precise simulation of electromagnetic calorimeter showers using a Wasserstein Generative Adversarial Network},
	volume = {3},
	issn = {2510-2036, 2510-2044},
	url = {http://arxiv.org/abs/1807.01954},
	doi = {10.1007/s41781-018-0019-7},
	abstract = {Simulations of particle showers in calorimeters are computationally time-consuming, as they have to reproduce both energy depositions and their considerable fluctuations. A new approach to ultra-fast simulations are generative models where all calorimeter energy depositions are generated simultaneously. We use {GEANT}4 simulations of an electron beam impinging on a multi-layer electromagnetic calorimeter for adversarial training of a generator network and a critic network guided by the Wasserstein distance. The generator is constraint during the training such that the generated showers show the expected dependency on the initial energy and the impact position. It produces realistic calorimeter energy depositions, fluctuations and correlations which we demonstrate in distributions of typical calorimeter observables. In most aspects, we observe that generated calorimeter showers reach the level of showers as simulated with the {GEANT}4 program.},
	pages = {4},
	number = {1},
	journaltitle = {Computing and Software for Big Science},
	shortjournal = {Comput Softw Big Sci},
	author = {Erdmann, Martin and Glombitza, Jonas and Quast, Thorben},
	urldate = {2021-01-13},
	date = {2019-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.01954},
	keywords = {High Energy Physics - Experiment, Physics - Instrumentation and Detectors},
	file = {Erdmann et al. - 2019 - Precise simulation of electromagnetic calorimeter .pdf:/home/md618/Zotero/storage/E64JV7V5/Erdmann et al. - 2019 - Precise simulation of electromagnetic calorimeter .pdf:application/pdf},
}

@article{otten_event_2019,
	title = {Event Generation and Statistical Sampling for Physics with Deep Generative Models and a Density Information Buffer},
	url = {http://arxiv.org/abs/1901.00875},
	abstract = {We present a study for the generation of events from a physical process with deep generative models. The simulation of physical processes requires not only the production of physical events, but also to ensure these events occur with the correct frequencies. We investigate the feasibility of learning the event generation and the frequency of occurrence with Generative Adversarial Networks ({GANs}) and Variational Autoencoders ({VAEs}) to produce events like Monte Carlo generators. We study three processes: a simple two-body decay, the processes \$e{\textasciicircum}+e{\textasciicircum}-{\textbackslash}to Z {\textbackslash}to l{\textasciicircum}+l{\textasciicircum}-\$ and \$p p {\textbackslash}to t{\textbackslash}bar\{t\} \$ including the decay of the top quarks and a simulation of the detector response. We find that the tested {GAN} architectures and the standard {VAE} are not able to learn the distributions precisely. By buffering density information of encoded Monte Carlo events given the encoder of a {VAE} we are able to construct a prior for the sampling of new events from the decoder that yields distributions that are in very good agreement with real Monte Carlo events and are generated several orders of magnitude faster. Applications of this work include generic density estimation and sampling, targeted event generation via a principal component analysis of encoded ground truth data, anomaly detection and more efficient importance sampling, e.g. for the phase space integration of matrix elements in quantum field theories.},
	journaltitle = {{arXiv}:1901.00875 [hep-ex, physics:hep-ph, physics:physics]},
	author = {Otten, Sydney and Caron, Sascha and de Swart, Wieske and van Beekveld, Melissa and Hendriks, Luc and van Leeuwen, Caspar and Podareanu, Damian and de Austri, Roberto Ruiz and Verheyen, Rob},
	urldate = {2021-01-13},
	date = {2019-12-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1901.00875},
	keywords = {High Energy Physics - Phenomenology, High Energy Physics - Experiment, Physics - Data Analysis, Statistics and Probability},
	file = {Otten et al. - 2019 - Event Generation and Statistical Sampling for Phys.pdf:/home/md618/Zotero/storage/9F3FXURZ/Otten et al. - 2019 - Event Generation and Statistical Sampling for Phys.pdf:application/pdf},
}

@online{noauthor_what_nodate,
	title = {What are temporal convolutional neural networks?},
	url = {https://www.quora.com/What-are-temporal-convolutional-neural-networks},
	abstract = {Answer: Temporal Convolutional Networks, or simply {TCN} is a variation over Convolutional Neural Networks for sequence modelling tasks. Rather, it’s quite a descriptive term for a family of architectures.

Motivation:

 * {TCNs} exhibit longer memory than recurrent architectures with the same capaci...},
	titleaddon = {Quora},
	urldate = {2021-01-15},
	langid = {english},
	file = {Snapshot:/home/md618/Zotero/storage/SILS4YTB/What-are-temporal-convolutional-neural-networks.html:text/html},
}

@article{choi_generating_2018,
	title = {Generating Multi-label Discrete Patient Records using Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1703.06490},
	abstract = {Access to electronic health record ({EHR}) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of {EHR} data. Sharing synthetic {EHR} data could mitigate risk.},
	journaltitle = {{arXiv}:1703.06490 [cs]},
	author = {Choi, Edward and Biswal, Siddharth and Malin, Bradley and Duke, Jon and Stewart, Walter F. and Sun, Jimeng},
	urldate = {2021-01-19},
	date = {2018-01-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.06490},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Choi et al. - 2018 - Generating Multi-label Discrete Patient Records us.pdf:/home/md618/Zotero/storage/D9ILUI9N/Choi et al. - 2018 - Generating Multi-label Discrete Patient Records us.pdf:application/pdf},
}

@article{brenninkmeijer_generation_nodate,
	title = {On the Generation and Evaluation of Tabular Data using {GANs}},
	pages = {70},
	author = {Brenninkmeijer, Bauke},
	langid = {english},
	file = {Brenninkmeijer - On the Generation and Evaluation of Tabular Data u.pdf:/home/md618/Zotero/storage/3GK5DWGL/Brenninkmeijer - On the Generation and Evaluation of Tabular Data u.pdf:application/pdf},
}

@online{noauthor_neural_nodate,
	title = {neural network - Gumbel-Softmax trick vs Softmax with temperature},
	url = {https://datascience.stackexchange.com/questions/58376/gumbel-softmax-trick-vs-softmax-with-temperature},
	titleaddon = {Data Science Stack Exchange},
	urldate = {2021-01-20},
}

@article{kurach_large-scale_nodate,
	title = {A Large-Scale Study on Regularization and Normalization in {GANs}},
	abstract = {Generative adversarial networks ({GANs}) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a {GAN} is a notoriously challenging task and requires a signiﬁcant number of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of “tricks”. The success in many practical applications coupled with the lack of a measure to quantify the failure modes of {GANs} resulted in a plethora of proposed losses, regularization and normalization schemes, as well as neural architectures. In this work we take a sober view of the current state of {GANs} from a practical perspective. We discuss and evaluate common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on {TensorFlow} Hub.},
	pages = {10},
	author = {Kurach, Karol and Lucic, Mario and Zhai, Xiaohua and Michalski, Marcin and Gelly, Sylvain},
	langid = {english},
	file = {Kurach et al. - A Large-Scale Study on Regularization and Normaliz.pdf:/home/md618/Zotero/storage/MSAF4BNE/Kurach et al. - A Large-Scale Study on Regularization and Normaliz.pdf:application/pdf},
}

@article{buhmann_getting_2020,
	title = {Getting High: High Fidelity Simulation of High Granularity Calorimeters with High Speed},
	url = {http://arxiv.org/abs/2005.05334},
	shorttitle = {Getting High},
	abstract = {Accurate simulation of physical processes is crucial for the success of modern particle physics. However, simulating the development and interaction of particle showers with calorimeter detectors is a time consuming process and drives the computing needs of large experiments at the {LHC} and future colliders. Recently, generative machine learning models based on deep neural networks have shown promise in speeding up this task by several orders of magnitude. We investigate the use of a new architecture — the Bounded Information Bottleneck Autoencoder — for modelling electromagnetic showers in the central region of the {SiliconTungsten} calorimeter of the proposed International Large Detector. Combined with a novel second post-processing network, this approach achieves an accurate simulation of diﬀerential distributions including for the ﬁrst time the shape of the minimum-ionizing-particle peak compared to a full {GEANT}4 simulation for a highgranularity calorimeter with 27k simulated channels. The results are validated by comparing to established architectures. Our results further strengthen the case of using generative networks for fast simulation and demonstrate that physically relevant diﬀerential distributions can be described with high accuracy.},
	journaltitle = {{arXiv}:2005.05334 [hep-ex, physics:hep-ph, physics:physics]},
	author = {Buhmann, Erik and Diefenbacher, Sascha and Eren, Engin and Gaede, Frank and Kasieczka, Gregor and Korol, Anatolii and Krüger, Katja},
	urldate = {2021-01-23},
	date = {2020-07-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.05334},
	keywords = {High Energy Physics - Phenomenology, High Energy Physics - Experiment, Physics - Data Analysis, Statistics and Probability, Physics - Instrumentation and Detectors},
	file = {Buhmann et al. - 2020 - Getting High High Fidelity Simulation of High Gra.pdf:/home/md618/Zotero/storage/HVBARYFF/Buhmann et al. - 2020 - Getting High High Fidelity Simulation of High Gra.pdf:application/pdf},
}

@article{loshchilov_decoupled_2019,
	title = {Decoupled Weight Decay Regularization},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard {SGD} and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with {SGD} with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in {TensorFlow} and {PyTorch}; the complete source code for our experiments is available at https://github.com/loshchil/{AdamW}-and-{SGDW}},
	journaltitle = {{arXiv}:1711.05101 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2021-01-23},
	date = {2019-01-04},
	eprinttype = {arxiv},
	eprint = {1711.05101},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/2YN6GH9Q/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/DL9L6LCL/1711.html:text/html},
}

@article{cote_synthesizing_2020,
	title = {Synthesizing Property \& Casualty Ratemaking Datasets using Generative Adversarial Networks},
	url = {http://arxiv.org/abs/2008.06110},
	abstract = {Due to conﬁdentiality issues, it can be diﬃcult to access or share interesting datasets for methodological development in actuarial science, or other ﬁelds where personal data are important. We show how to design three diﬀerent types of generative adversarial networks ({GANs}) that can build a synthetic insurance dataset from a conﬁdential original dataset. The goal is to obtain synthetic data that no longer contains sensitive information but still has the same structure as the original dataset and retains the multivariate relationships. In order to adequately model the speciﬁc characteristics of insurance data, we use {GAN} architectures adapted for multi-categorical data: a Wassertein {GAN} with gradient penalty ({MC}-{WGAN}-{GP}), a conditional tabular {GAN} ({CTGAN}) and a Mixed Numerical and Categorical Diﬀerentially Private {GAN} ({MNCDP}-{GAN}). For transparency, the approaches are illustrated using a public dataset, the French motor third party liability data. We compare the three diﬀerent {GANs} on various aspects: ability to reproduce the original data structure and predictive models, privacy, and ease of use. We ﬁnd that the {MC}-{WGAN}-{GP} synthesizes the best data, the {CTGAN} is the easiest to use, and the {MNCDP}-{GAN} guarantees diﬀerential privacy.},
	journaltitle = {{arXiv}:2008.06110 [cs, stat]},
	author = {Cote, Marie-Pier and Hartman, Brian and Mercier, Olivier and Meyers, Joshua and Cummings, Jared and Harmon, Elijah},
	urldate = {2021-01-24},
	date = {2020-08-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.06110},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Cote et al. - 2020 - Synthesizing Property & Casualty Ratemaking Datase.pdf:/home/md618/Zotero/storage/YL5GZ9BL/Cote et al. - 2020 - Synthesizing Property & Casualty Ratemaking Datase.pdf:application/pdf},
}

@article{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	journaltitle = {{arXiv}:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2021-01-25},
	date = {2014-06-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:/home/md618/Zotero/storage/CZCT6NSX/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf},
}

@article{ahdida_fast_2019,
	title = {Fast simulation of muons produced at the {SHiP} experiment using Generative Adversarial Networks},
	volume = {14},
	issn = {1748-0221},
	url = {https://iopscience.iop.org/article/10.1088/1748-0221/14/11/P11028},
	doi = {10.1088/1748-0221/14/11/P11028},
	pages = {P11028--P11028},
	number = {11},
	journaltitle = {Journal of Instrumentation},
	shortjournal = {J. Inst.},
	author = {Ahdida, C. and Albanese, R. and Alexandrov, A. and Anokhina, A. and Aoki, S. and Arduini, G. and Atkin, E. and Azorskiy, N. and Back, J.J. and Bagulya, A. and Santos, F. Baaltasar Dos and Baranov, A. and Bardou, F. and Barker, G.J. and Battistin, M. and Bauche, J. and Bay, A. and Bayliss, V. and Bencivenni, G. and Berdnikov, A.Y. and Berdnikov, Y.A. and Berezkina, I. and Bertani, M. and Betancourt, C. and Bezshyiko, I. and Bezshyyko, O. and Bick, D. and Bieschke, S. and Blanco, A. and Boehm, J. and Bogomilov, M. and Bondarenko, K. and Bonivento, W.M. and Borburgh, J. and Boyarsky, A. and Brenner, R. and Breton, D. and Brundler, R. and Bruschi, M. and Büscher, V. and Buonaura, A. and Buontempo, S. and Cadeddu, S. and Calcaterra, A. and Calviani, M. and Campanelli, M. and Casolino, M. and Charitonidis, N. and Chau, P. and Chauveau, J. and Chepurnov, A. and Chernyavskiy, M. and Choi, K.-Y. and Chumakov, A. and Ciambrone, P. and Congedo, L. and Cornelis, K. and Cristinziani, M. and Crupano, A. and Dallavalle, G.M. and Datwyler, A. and D'Ambrosio, N. and D'Appollonio, G. and Saraiva, J. De Carvalho and Lellis, G. De and de Magistris, M. and Roeck, A. De and Serio, M. De and Simone, D. De and Dedenko, L. and Dergachev, P. and Crescenzo, A. Di and Marco, N. Di and Dib, C. and Dijkstra, H. and Dipinto, P. and Dmitrenko, V. and Dmitrievskiy, S. and Dougherty, L.A. and Dolmatov, A. and Domenici, D. and Donskov, S. and Drohan, V. and Dubreuil, A. and Ehlert, M. and Enik, T. and Etenko, A. and Fabbri, F. and Fabbri, L. and Fabich, A. and Fedin, O. and Fedotovs, F. and Felici, G. and Ferro-Luzzi, M. and Filippov, K. and Fini, R.A. and Fonte, P. and Franco, C. and Fraser, M. and Fresa, R. and Froeschl, R. and Fukuda, T. and Galati, G. and Gall, J. and Gatignon, L. and Gavrilov, G. and Gentile, V. and Gerlach, S. and Goddard, B. and Golinka-Bezshyyko, L. and Golovatiuk, A. and Golubkov, D. and Golutvin, A. and Gorbounov, P. and Gorbunov, D. and Gorbunov, S. and Gorkavenko, V. and Gornushkin, Y. and Gorshenkov, M. and Grachev, V. and Grandchamp, A.L. and Granich, G. and Graverini, E. and Grenard, J.-L. and Grenier, D. and Grichine, V. and Gruzinskii, N. and Guler, A. M. and Guz, Yu. and Haefeli, G.J. and Hagner, C. and Hakobyan, H. and Harris, I.W. and Herwijnen, E. van and Hessler, C. and Hollnagel, A. and Hosseini, B. and Hushchyn, M. and Iaselli, G. and Iuliano, A. and Ivantchenko, V. and Jacobsson, R. and Joković, D. and Jonker, M. and Kadenko, I. and Kain, V. and Kaiser, B. and Kamiscioglu, C. and Kershaw, K. and Khabibullin, M. and Khalikov, E. and Khaustov, G. and Khoriauli, G. and Khotyantsev, A. and Kim, S.H. and Kim, Y.G. and Kim, V. and Kitagawa, N. and Ko, J.-W. and Kodama, K. and Kolesnikov, A. and Kolev, D.I. and Kolosov, V. and Komatsu, M. and Kondrateva, N. and Kono, A. and Konovalova, N. and Kormannshaus, S. and Korol, I. and Korol'ko, I. and Korzenev, A. and Kostyukhin, V. and Platia, E. Koukovini and Kovalenko, S. and Krasilnikova, I. and Kudenko, Y. and Kurbatov, E. and Kurbatov, P. and Kurochka, V. and Kuznetsova, E. and Lacker, H.M. and Lamont, M. and Lanfranchi, G. and Lantwin, O. and Lauria, A. and Lee, K.S. and Lee, K.Y. and Lévy, J.-M. and Loschiavo, V.P. and Lopes, L. and Sola, E. Lopez and Lyubovitskij, V. and Maalmi, J. and Magnan, A. and Maleev, V. and Malinin, A. and Manabe, Y. and Managadze, A.K. and Manfredi, M. and Marsh, S. and Marshall, A.M. and Mefodev, A. and Mermod, P. and Miano, A. and Mikado, S. and Mikhaylov, Yu. and Milstead, D.A. and Mineev, O. and Montanari, A. and Montesi, M.C. and Morishima, K. and Movchan, S. and Muttoni, Y. and Naganawa, N. and Nakamura, M. and Nakano, T. and Nasybulin, S. and Ninin, P. and Nishio, A. and Novikov, A. and Obinyakov, B. and Ogawa, S. and Okateva, N. and Opitz, B. and Osborne, J. and Ovchynnikov, M. and Owtscharenko, N. and Owen, P.H. and Pacholek, P. and Paoloni, A. and Park, B.D. and Park, S.K. and Pastore, A. and Patel, M. and Pereyma, D. and Perillo-Marcone, A. and Petkov, G.L. and Petridis, K. and Petrov, A. and Podgrudkov, D. and Poliakov, V. and Polukhina, N. and Prieto, J. Prieto and Prokudin, M. and Prota, A. and Quercia, A. and Rademakers, A. and Rakai, A. and Ratnikov, F. and Rawlings, T. and Redi, F. and Ricciardi, S. and Rinaldesi, M. and Rodin, Volodymyr and Rodin, Viktor and Robbe, P. and Cavalcante, A.B. Rodrigues and Roganova, T. and Rokujo, H. and Rosa, G. and Rovelli, T. and Ruchayskiy, O. and Ruf, T. and Samoylenko, V. and Samsonov, V. and Galan, F. Sanchez and Diaz, P. Santos and Ull, A. Sanz and Saputi, A. and Sato, O. and Savchenko, E.S. and Schliwinski, J.S. and Schmidt-Parzefall, W. and Serra, N. and Sgobba, S. and Shadura, O. and Shakin, A. and Shaposhnikov, M. and Shatalov, P. and Shchedrina, T. and Shchutska, L. and Shevchenko, V. and Shibuya, H. and Shihora, L. and Shirobokov, S. and Shustov, A. and Silverstein, S.B. and Simone, S. and Simoniello, R. and Skorokhvatov, M. and Smirnov, S. and Sohn, J.Y. and Sokolenko, A. and Solodko, E. and Starkov, N. and Stoel, L. and Storaci, B. and Stramaglia, M.E. and Sukhonos, D. and Suzuki, Y. and Takahashi, S. and Tastet, J.L. and Teterin, P. and Naing, S. Than and Timiryasov, I. and Tioukov, V. and Tommasini, D. and Torii, M. and Tosi, N. and Treille, D. and Tsenov, R. and Ulin, S. and Ustyuzhanin, A. and Uteshev, Z. and Vankova-Kirilova, G. and Vannucci, F. and Venkova, P. and Venturi, V. and Vilchinski, S. and Villa, M. and Vincke, Heinz and Vincke, Helmut and Visone, C. and Vlasik, K. and Volkov, A. and Voronkov, R. and Waasen, S. van and Wanke, R. and Wertelaers, P. and Woo, J.-K. and Wurm, M. and Xella, S. and Yilmaz, D. and Yilmazer, A.U. and Yoon, C.S. and Zarubin, P. and Zarubina, I. and Zaytsev, Yu.},
	urldate = {2021-01-25},
	date = {2019-11-27},
	langid = {english},
	file = {Ahdida et al. - 2019 - Fast simulation of muons produced at the SHiP expe.pdf:/home/md618/Zotero/storage/ZZK3DMJG/Ahdida et al. - 2019 - Fast simulation of muons produced at the SHiP expe.pdf:application/pdf},
}

@article{butter_how_2020,
	title = {How to {GAN} Event Subtraction},
	url = {http://arxiv.org/abs/1912.08824},
	doi = {10.21468/SciPostPhysCore.3.2.009},
	abstract = {Subtracting event samples is a common task in {LHC} simulation and analysis, and standard solutions tend to be ineﬃcient. We employ generative adversarial networks to produce new event samples with a phase space distribution corresponding to added or subtracted input samples. We ﬁrst illustrate for a toy example how such a network beats the statistical limitations of the training data. We then show how such a network can be used to subtract background events or to include non-local collinear subtraction events at the level of unweighted 4-vector events.},
	journaltitle = {{arXiv}:1912.08824 [hep-ph]},
	author = {Butter, Anja and Plehn, Tilman and Winterhalder, Ramon},
	urldate = {2021-01-25},
	date = {2020-10-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.08824},
	keywords = {High Energy Physics - Phenomenology},
	file = {Butter et al. - 2020 - How to GAN Event Subtraction.pdf:/home/md618/Zotero/storage/SW73L5SS/Butter et al. - 2020 - How to GAN Event Subtraction.pdf:application/pdf},
}

@article{kendrick_anysize_2020,
	title = {Anysize {GAN}: A solution to the image-warping problem},
	url = {http://arxiv.org/abs/2003.03233},
	shorttitle = {Anysize {GAN}},
	abstract = {We propose a new type of General Adversarial Network ({GAN}) to resolve a common issue with Deep Learning. We develop a novel architecture that can be applied to existing latent vector based {GAN} structures that allows them to generate on-the-ﬂy images of any size. Existing {GAN} for image generation requires uniform images of matching dimensions. However, publicly available datasets, such as {ImageNet} contain thousands of diﬀerent sizes. Resizing image causes deformations and changing the image data, whereas as our network does not require this preprocessing step. We make signiﬁcant changes to the standard data loading techniques to enable any size image to be loaded for training. We also modify the network in two ways, by adding multiple inputs and a novel dynamic resizing layer. Finally we make adjustments to the discriminator to work on multiple resolutions. These changes can allow multiple resolution datasets to be trained on without any resizing, if memory allows. We validate our results on the {ISIC} 2019 skin lesion dataset. We demonstrate our method can successfully generate realistic images at diﬀerent sizes without issue, preserving and understanding spatial relationships, while maintaining feature relationships. We will release the source codes upon paper acceptance.},
	journaltitle = {{arXiv}:2003.03233 [cs, eess]},
	author = {Kendrick, Connah and Gillespie, David and Yap, Moi Hoon},
	urldate = {2021-01-28},
	date = {2020-07-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.03233},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Kendrick et al. - 2020 - Anysize GAN A solution to the image-warping probl.pdf:/home/md618/Zotero/storage/RG52SGUI/Kendrick et al. - 2020 - Anysize GAN A solution to the image-warping probl.pdf:application/pdf},
}

@article{de_oliveira_learning_2017,
	title = {Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis},
	volume = {1},
	issn = {2510-2036, 2510-2044},
	url = {http://arxiv.org/abs/1701.05927},
	doi = {10.1007/s41781-017-0004-6},
	shorttitle = {Learning Particle Physics by Example},
	abstract = {We provide a bridge between generative modeling in the Machine Learning community and simulated physical processes in High Energy Particle Physics by applying a novel Generative Adversarial Network ({GAN}) architecture to the production of jet images – 2D representations of energy depositions from particles interacting with a calorimeter. We propose a simple architecture, the Location-Aware Generative Adversarial Network, that learns to produce realistic radiation patterns from simulated high energy particle collisions. The pixel intensities of {GAN}-generated images faithfully span over many orders of magnitude and exhibit the desired low-dimensional physical properties (i.e., jet mass, n-subjettiness, etc.). We shed light on limitations, and provide a novel empirical validation of image quality and validity of {GAN}-produced simulations of the natural world. This work provides a base for further explorations of {GANs} for use in faster simulation in High Energy Particle Physics.},
	pages = {4},
	number = {1},
	journaltitle = {Computing and Software for Big Science},
	shortjournal = {Comput Softw Big Sci},
	author = {de Oliveira, Luke and Paganini, Michela and Nachman, Benjamin},
	urldate = {2021-01-28},
	date = {2017-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1701.05927},
	keywords = {High Energy Physics - Experiment, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
	file = {de Oliveira et al. - 2017 - Learning Particle Physics by Example Location-Awa.pdf:/home/md618/Zotero/storage/VV8LZY5X/de Oliveira et al. - 2017 - Learning Particle Physics by Example Location-Awa.pdf:application/pdf},
}

@article{musella_fast_2018,
	title = {Fast and accurate simulation of particle detectors using generative adversarial networks},
	volume = {2},
	issn = {2510-2036, 2510-2044},
	url = {http://arxiv.org/abs/1805.00850},
	doi = {10.1007/s41781-018-0015-y},
	abstract = {Deep generative models parametrised by neural networks have recently started to provide accurate results in modeling natural images. In particular, generative adversarial networks provide an unsupervised solution to this problem. In this work we apply this kind of technique to the simulation of particle-detector response to hadronic jets. We show that deep neural networks can achieve high-ﬁdelity in this task, while attaining a speed increase of several orders of magnitude with respect to traditional algorithms.},
	pages = {8},
	number = {1},
	journaltitle = {Computing and Software for Big Science},
	shortjournal = {Comput Softw Big Sci},
	author = {Musella, Pasquale and Pandolfi, Francesco},
	urldate = {2021-01-28},
	date = {2018-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.00850},
	keywords = {High Energy Physics - Phenomenology, High Energy Physics - Experiment, Physics - Data Analysis, Statistics and Probability},
	file = {Musella and Pandolfi - 2018 - Fast and accurate simulation of particle detectors.pdf:/home/md618/Zotero/storage/I4GD2UMW/Musella and Pandolfi - 2018 - Fast and accurate simulation of particle detectors.pdf:application/pdf},
}

@article{erdmann_generating_2018,
	title = {Generating and refining particle detector simulations using the Wasserstein distance in adversarial networks},
	url = {http://arxiv.org/abs/1802.03325},
	abstract = {We use adversarial network architectures together with the Wasserstein distance to generate or reﬁne simulated detector data. The data reﬂect twodimensional projections of spatially distributed signal patterns with a broad spectrum of applications. As an example, we use an observatory to detect cosmic rayinduced air showers with a ground-based array of particle detectors. First we investigate a method of generating detector patterns with variable signal strengths while constraining the primary particle energy. We then present a technique to reﬁne simulated time traces of detectors to match corresponding data distributions. With this method we demonstrate that training a deep network with reﬁned data-like signal traces leads to a more precise energy reconstruction of data events compared to training with the originally simulated traces.},
	journaltitle = {{arXiv}:1802.03325 [astro-ph, physics:hep-ex]},
	author = {Erdmann, Martin and Geiger, Lukas and Glombitza, Jonas and Schmidt, David},
	urldate = {2021-01-29},
	date = {2018-02-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1802.03325},
	keywords = {High Energy Physics - Experiment, Astrophysics - Instrumentation and Methods for Astrophysics},
	file = {Erdmann et al. - 2018 - Generating and refining particle detector simulati.pdf:/home/md618/Zotero/storage/BRC7JNRM/Erdmann et al. - 2018 - Generating and refining particle detector simulati.pdf:application/pdf},
}

@article{the_comet_collaboration_comet_2020,
	title = {{COMET} Phase-I technical design report},
	volume = {2020},
	issn = {2050-3911},
	url = {https://academic.oup.com/ptep/article/doi/10.1093/ptep/ptz125/5805094},
	doi = {10.1093/ptep/ptz125},
	abstract = {Abstract
            The Technical Design for the {COMET} Phase-I experiment is presented in this paper. {COMET} is an experiment at J-{PARC}, Japan, which will search for neutrinoless conversion of muons into electrons in the field of an aluminum nucleus (\${\textbackslash}mu\$–\$e\$ conversion, \${\textbackslash}mu{\textasciicircum}\{-\}N {\textbackslash}rightarrow e{\textasciicircum}\{-\}N\$); a lepton flavor-violating process. The experimental sensitivity goal for this process in the Phase-I experiment is \$3.1{\textbackslash}times10{\textasciicircum}\{-15\}\$, or 90\% upper limit of a branching ratio of \$7{\textbackslash}times 10{\textasciicircum}\{-15\}\$, which is a factor of 100 improvement over the existing limit. The expected number of background events is 0.032. To achieve the target sensitivity and background level, the 3.2 {kW} 8 {GeV} proton beam from J-{PARC} will be used. Two types of detectors, {CyDet} and {StrECAL}, will be used for detecting the \${\textbackslash}mu\$–\$e\$ conversion events, and for measuring the beam-related background events in view of the Phase-{II} experiment, respectively. Results from simulation on signal and background estimations are also described.},
	pages = {033C01},
	number = {3},
	journaltitle = {Progress of Theoretical and Experimental Physics},
	author = {{The COMET Collaboration} and Abramishvili, R and Adamov, G and Akhmetshin, R R and Allin, A and Angélique, J C and Anishchik, V and Aoki, M and Aznabayev, D and Bagaturia, I and Ban, G and Ban, Y and Bauer, D and Baygarashev, D and Bondar, A E and Cârloganu, C and Carniol, B and Chau, T T and Chen, J K and Chen, S J and Cheung, Y E and da Silva, W and Dauncey, P D and Densham, C and Devidze, G and Dornan, P and Drutskoy, A and Duginov, V and Eguchi, Y and Epshteyn, L B and Evtoukhovitch, P and Fayer, S and Fedotovich, G V and Finger Jr, M and Finger, M and Fujii, Y and Fukao, Y and Gabriel, J L and Gay, P and Gillies, E and Grigoriev, D N and Gritsay, K and Hai, V H and Hamada, E and Hashim, I H and Hashimoto, S and Hayashi, O and Hayashi, T and Hiasa, T and Ibrahim, Z A and Igarashi, Y and Ignatov, F V and Iio, M and Ishibashi, K and Issadykov, A and Itahashi, T and Jansen, A and Jiang, X S and Jonsson, P and Kachelhoffer, T and Kalinnikov, V and Kaneva, E and Kapusta, F and Katayama, H and Kawagoe, K and Kawashima, R and Kazak, N and Kazanin, V F and Kemularia, O and Khvedelidze, A and Koike, M and Kormoll, T and Kozlov, G A and Kozyrev, A N and Kravchenko, M and Krikler, B and Kumsiashvili, G and Kuno, Y and Kuriyama, Y and Kurochkin, Y and Kurup, A and Lagrange, B and Lai, J and Lee, M J and Li, H B and Litchfield, R P and Li, W G and Loan, T and Lomidze, D and Lomidze, I and Loveridge, P and Macharashvili, G and Makida, Y and Mao, Y J and Markin, O and Matsuda, Y and Melkadze, A and Melnik, A and Mibe, T and Mihara, S and Miyamoto, N and Miyazaki, Y and Mohamad Idris, F and Azmi, K A Mohamed Kamal and Moiseenko, A and Moritsu, M and Mori, Y and Motoishi, T and Nakai, H and Nakai, Y and Nakamoto, T and Nakamura, Y and Nakatsugawa, Y and Nakazawa, Y and Nash, J and Natori, H and Niess, V and Nioradze, M and Nishiguchi, H and Noguchi, K and Numao, T and O’Dell, J and Ogitsu, T and Ohta, S and Oishi, K and Okamoto, K and Okamura, T and Okinaka, K and Omori, C and Ota, T and Pasternak, J and Paulau, A and Picters, D and Ponariadov, V and Quémener, G and Ruban, A A and Rusinov, V and Sabirov, B and Sakamoto, H and Sarin, P and Sasaki, K and Sato, A and Sato, J and Semertzidis, Y K and Shigyo, N and Shoukavy, Dz and Slunecka, M and Stöckinger, D and Sugano, M and Tachimoto, T and Takayanagi, T and Tanaka, M and Tang, J and Tao, C V and Teixeira, A M and Tevzadze, Y and Thanh, T and Tojo, J and Tolmachev, S S and Tomasek, M and Tomizawa, M and Toriashvili, T and Trang, H and Trekov, I and Tsamalaidze, Z and Tsverava, N and Uchida, T and Uchida, Y and Ueno, K and Velicheva, E and Volkov, A and Vrba, V and Abdullah, W A T Wan and Warin-Charpentier, P and Wong, M L and Wong, T S and Wu, C and Xing, T Y and Yamaguchi, H and Yamamoto, A and Yamanaka, M and Yamane, T and Yang, Y and Yano, T and Yao, W C and Yeo, B and Yoshida, H and Yoshida, M and Yoshioka, T and Yuan, Y and Yudin, Yu V and Zdorovets, M V and Zhang, J and Zhang, Y and Zuber, K},
	urldate = {2021-01-29},
	date = {2020-03-01},
	langid = {english},
	file = {The COMET Collaboration et al. - 2020 - COMET Phase-I technical design report.pdf:/home/md618/Zotero/storage/IFDUFLGP/The COMET Collaboration et al. - 2020 - COMET Phase-I technical design report.pdf:application/pdf},
}

@article{Bertl:2006up,
    author = "Bertl, Wilhelm H. and others",
    collaboration = "SINDRUM II",
    title = "{A Search for muon to electron conversion in muonic gold}",
    doi = "10.1140/epjc/s2006-02582-x",
    journal = "Eur. Phys. J. C",
    volume = "47",
    pages = "337--346",
    year = "2006"
}


@article{kansal_graph_2021,
	title = {Graph Generative Adversarial Networks for Sparse Data Generation in High Energy Physics},
	url = {http://arxiv.org/abs/2012.00173},
	abstract = {We develop a graph generative adversarial network to generate sparse data sets like those produced at the {CERN} Large Hadron Collider ({LHC}). We demonstrate this approach by training on and generating sparse representations of {MNIST} handwritten digit images and jets of particles in proton-proton collisions like those at the {LHC}. We find the model successfully generates sparse {MNIST} digits and particle jet data. We quantify agreement between real and generated data with a graph-based Fr{\textbackslash}'echet Inception distance, and the particle and jet feature-level 1-Wasserstein distance for the {MNIST} and jet datasets respectively.},
	journaltitle = {{arXiv}:2012.00173 [hep-ex, physics:hep-ph, physics:physics]},
	author = {Kansal, Raghav and Duarte, Javier and Orzari, Breno and Tomei, Thiago and Pierini, Maurizio and Touranakou, Mary and Vlimant, Jean-Roch and Gunopulos, Dimitrios},
	urldate = {2021-02-03},
	date = {2021-01-30},
	eprinttype = {arxiv},
	eprint = {2012.00173},
	note = {version: 3},
	keywords = {High Energy Physics - Phenomenology, High Energy Physics - Experiment, Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/md618/Zotero/storage/LHUHVWWF/Kansal et al. - 2021 - Graph Generative Adversarial Networks for Sparse D.pdf:application/pdf;arXiv.org Snapshot:/home/md618/Zotero/storage/8JAY7ZBX/2012.html:text/html},
}

@article{lin_pacgan_2018,
	title = {{PacGAN}: The power of two samples in generative adversarial networks},
	url = {http://arxiv.org/abs/1712.04086},
	shorttitle = {{PacGAN}},
	abstract = {Generative adversarial networks ({GANs}) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable recent improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the main focus of several recent advances in {GANs}. Yet there is little understanding of why mode collapse happens and why recently proposed approaches are able to mitigate mode collapse. We propose a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artiﬁcially generated. We borrow analysis tools from binary hypothesis testing—in particular the seminal result of Blackwell [6]—to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides signiﬁcant improvements in practice as well.},
	journaltitle = {{arXiv}:1712.04086 [cs, math, stat]},
	author = {Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
	urldate = {2021-02-04},
	date = {2018-11-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.04086},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {Lin et al. - 2018 - PacGAN The power of two samples in generative adv.pdf:/home/md618/Zotero/storage/V7R5U7ER/Lin et al. - 2018 - PacGAN The power of two samples in generative adv.pdf:application/pdf},
}

@inproceedings{45822,
title	= {Categorical Reparameterization with Gumbel-Softmax},
author	= {Eric Jang and Shixiang Gu and Ben Poole},
year	= {2017},
URL	= {https://arxiv.org/abs/1611.01144}
}

% Muon g-2 FNAL
@article{PhysRevLett.126.141801,
  title = {Measurement of the Positive Muon Anomalous Magnetic Moment to 0.46 ppm},
  author = {Abi, B. and Albahri, T. and Al-Kilani, S. and Allspach, D. and Alonzi, L. P. and Anastasi, A. and Anisenkov, A. and Azfar, F. and Badgley, K. and Bae\ss{}ler, S. and Bailey, I. and Baranov, V. A. and Barlas-Yucel, E. and Barrett, T. and Barzi, E. and Basti, A. and Bedeschi, F. and Behnke, A. and Berz, M. and Bhattacharya, M. and Binney, H. P. and Bjorkquist, R. and Bloom, P. and Bono, J. and Bottalico, E. and Bowcock, T. and Boyden, D. and Cantatore, G. and Carey, R. M. and Carroll, J. and Casey, B. C. K. and Cauz, D. and Ceravolo, S. and Chakraborty, R. and Chang, S. P. and Chapelain, A. and Chappa, S. and Charity, S. and Chislett, R. and Choi, J. and Chu, Z. and Chupp, T. E. and Convery, M. E. and Conway, A. and Corradi, G. and Corrodi, S. and Cotrozzi, L. and Crnkovic, J. D. and Dabagov, S. and De Lurgio, P. M. and Debevec, P. T. and Di Falco, S. and Di Meo, P. and Di Sciascio, G. and Di Stefano, R. and Drendel, B. and Driutti, A. and Duginov, V. N. and Eads, M. and Eggert, N. and Epps, A. and Esquivel, J. and Farooq, M. and Fatemi, R. and Ferrari, C. and Fertl, M. and Fiedler, A. and Fienberg, A. T. and Fioretti, A. and Flay, D. and Foster, S. B. and Friedsam, H. and Frle\ifmmode \check{z}\else \v{z}\fi{}, E. and Froemming, N. S. and Fry, J. and Fu, C. and Gabbanini, C. and Galati, M. D. and Ganguly, S. and Garcia, A. and Gastler, D. E. and George, J. and Gibbons, L. K. and Gioiosa, A. and Giovanetti, K. L. and Girotti, P. and Gohn, W. and Gorringe, T. and Grange, J. and Grant, S. and Gray, F. and Haciomeroglu, S. and Hahn, D. and Halewood-Leagas, T. and Hampai, D. and Han, F. and Hazen, E. and Hempstead, J. and Henry, S. and Herrod, A. T. and Hertzog, D. W. and Hesketh, G. and Hibbert, A. and Hodge, Z. and Holzbauer, J. L. and Hong, K. W. and Hong, R. and Iacovacci, M. and Incagli, M. and Johnstone, C. and Johnstone, J. A. and Kammel, P. and Kargiantoulakis, M. and Karuza, M. and Kaspar, J. and Kawall, D. and Kelton, L. and Keshavarzi, A. and Kessler, D. and Khaw, K. S. and Khechadoorian, Z. and Khomutov, N. V. and Kiburg, B. and Kiburg, M. and Kim, O. and Kim, S. C. and Kim, Y. I. and King, B. and Kinnaird, N. and Korostelev, M. and Kourbanis, I. and Kraegeloh, E. and Krylov, V. A. and Kuchibhotla, A. and Kuchinskiy, N. A. and Labe, K. R. and LaBounty, J. and Lancaster, M. and Lee, M. J. and Lee, S. and Leo, S. and Li, B. and Li, D. and Li, L. and Logashenko, I. and Lorente Campos, A. and Luc\`a, A. and Lukicov, G. and Luo, G. and Lusiani, A. and Lyon, A. L. and MacCoy, B. and Madrak, R. and Makino, K. and Marignetti, F. and Mastroianni, S. and Maxfield, S. and McEvoy, M. and Merritt, W. and Mikhailichenko, A. A. and Miller, J. P. and Miozzi, S. and Morgan, J. P. and Morse, W. M. and Mott, J. and Motuk, E. and Nath, A. and Newton, D. and Nguyen, H. and Oberling, M. and Osofsky, R. and Ostiguy, J.-F. and Park, S. and Pauletta, G. and Piacentino, G. M. and Pilato, R. N. and Pitts, K. T. and Plaster, B. and Po\ifmmode \check{c}\else \v{c}\fi{}ani\ifmmode \acute{c}\else \'{c}\fi{}, D. and Pohlman, N. and Polly, C. C. and Popovic, M. and Price, J. and Quinn, B. and Raha, N. and Ramachandran, S. and Ramberg, E. and Rider, N. T. and Ritchie, J. L. and Roberts, B. L. and Rubin, D. L. and Santi, L. and Sathyan, D. and Schellman, H. and Schlesier, C. and Schreckenberger, A. and Semertzidis, Y. K. and Shatunov, Y. M. and Shemyakin, D. and Shenk, M. and Sim, D. and Smith, M. W. and Smith, A. and Soha, A. K. and Sorbara, M. and St\"ockinger, D. and Stapleton, J. and Still, D. and Stoughton, C. and Stratakis, D. and Strohman, C. and Stuttard, T. and Swanson, H. E. and Sweetmore, G. and Sweigart, D. A. and Syphers, M. J. and Tarazona, D. A. and Teubner, T. and Tewsley-Booth, A. E. and Thomson, K. and Tishchenko, V. and Tran, N. H. and Turner, W. and Valetov, E. and Vasilkova, D. and Venanzoni, G. and Volnykh, V. P. and Walton, T. and Warren, M. and Weisskopf, A. and Welty-Rieger, L. and Whitley, M. and Winter, P. and Wolski, A. and Wormald, M. and Wu, W. and Yoshikawa, C.},
  collaboration = {Muon $g\ensuremath{-}2$ Collaboration},
  journal = {Phys. Rev. Lett.},
  volume = {126},
  issue = {14},
  pages = {141801},
  numpages = {11},
  year = {2021},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.126.141801},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.126.141801}
}

% Latest LHC-b lepton universality violation
@misc{lhcbcollaboration2021test,
      title={Test of lepton universality in beauty-quark decays}, 
      author={LHCb collaboration and R. Aaij and C. Abellán Beteta and T. Ackernley and B. Adeva and M. Adinolfi and H. Afsharnia and C. A. Aidala and S. Aiola and Z. Ajaltouni and S. Akar and J. Albrecht and F. Alessio and M. Alexander and A. Alfonso Albero and Z. Aliouche and G. Alkhazov and P. Alvarez Cartelle and S. Amato and Y. Amhis and L. An and L. Anderlini and A. Andreianov and M. Andreotti and F. Archilli and A. Artamonov and M. Artuso and K. Arzymatov and E. Aslanides and M. Atzeni and B. Audurier and S. Bachmann and M. Bachmayer and J. J. Back and P. Baladron Rodriguez and V. Balagura and W. Baldini and J. Baptista Leite and R. J. Barlow and S. Barsuk and W. Barter and M. Bartolini and F. Baryshnikov and J. M. Basels and G. Bassi and B. Batsukh and A. Battig and A. Bay and M. Becker and F. Bedeschi and I. Bediaga and A. Beiter and V. Belavin and S. Belin and V. Bellee and K. Belous and I. Belov and I. Belyaev and G. Bencivenni and E. Ben-Haim and A. Berezhnoy and R. Bernet and D. Berninghoff and H. C. Bernstein and C. Bertella and A. Bertolin and C. Betancourt and F. Betti and Ia. Bezshyiko and S. Bhasin and J. Bhom and L. Bian and M. S. Bieker and S. Bifani and P. Billoir and M. Birch and F. C. R. Bishop and A. Bitadze and A. Bizzeti and M. Bjørn and M. P. Blago and T. Blake and F. Blanc and S. Blusk and D. Bobulska and J. A. Boelhauve and O. Boente Garcia and T. Boettcher and A. Boldyrev and A. Bondar and N. Bondar and S. Borghi and M. Borisyak and M. Borsato and J. T. Borsuk and S. A. Bouchiba and T. J. V. Bowcock and A. Boyer and C. Bozzi and M. J. Bradley and S. Braun and A. Brea Rodriguez and M. Brodski and J. Brodzicka and A. Brossa Gonzalo and D. Brundu and A. Buonaura and C. Burr and A. Bursche and A. Butkevich and J. S. Butter and J. Buytaert and W. Byczynski and S. Cadeddu and H. Cai and R. Calabrese and L. Calefice and L. Calero Diaz and S. Cali and R. Calladine and M. Calvi and M. Calvo Gomez and P. Camargo Magalhaes and A. Camboni and P. Campana and A. F. Campoverde Quezada and S. Capelli and L. Capriotti and A. Carbone and G. Carboni and R. Cardinale and A. Cardini and I. Carli and P. Carniti and L. Carus and K. Carvalho Akiba and A. Casais Vidal and G. Casse and M. Cattaneo and G. Cavallero and S. Celani and J. Cerasoli and A. J. Chadwick and M. G. Chapman and M. Charles and Ph. Charpentier and G. Chatzikonstantinidis and C. A. Chavez Barajas and M. Chefdeville and C. Chen and S. Chen and A. Chernov and V. Chobanova and S. Cholak and M. Chrzaszcz and A. Chubykin and V. Chulikov and P. Ciambrone and M. F. Cicala and X. Cid Vidal and G. Ciezarek and P. E. L. Clarke and M. Clemencic and H. V. Cliff and J. Closier and J. L. Cobbledick and V. Coco and J. A. B. Coelho and J. Cogan and E. Cogneras and L. Cojocariu and P. Collins and T. Colombo and L. Congedo and A. Contu and N. Cooke and G. Coombs and G. Corti and C. M. Costa Sobral and B. Couturier and D. C. Craik and J. Crkovská and M. Cruz Torres and R. Currie and C. L. Da Silva and E. Dall'Occo and J. Dalseno and C. D'Ambrosio and A. Danilina and P. d'Argent and A. Davis and O. De Aguiar Francisco and K. De Bruyn and S. De Capua and M. De Cian and J. M. De Miranda and L. De Paula and M. De Serio and D. De Simone and P. De Simone and J. A. de Vries and C. T. Dean and D. Decamp and L. Del Buono and B. Delaney and H. -P. Dembinski and A. Dendek and V. Denysenko and D. Derkach and O. Deschamps and F. Desse and F. Dettori and B. Dey and P. Di Nezza and S. Didenko and L. Dieste Maronas and H. Dijkstra and V. Dobishuk and A. M. Donohoe and F. Dordei and A. C. dos Reis and L. Douglas and A. Dovbnya and A. G. Downes and K. Dreimanis and M. W. Dudek and L. Dufour and V. Duk and P. Durante and J. M. Durham and D. Dutta and A. Dziurda and A. Dzyuba and S. Easo and U. Egede and V. Egorychev and S. Eidelman and S. Eisenhardt and S. Ek-In and L. Eklund and S. Ely and A. Ene and E. Epple and S. Escher and J. Eschle and S. Esen and T. Evans and A. Falabella and J. Fan and Y. Fan and B. Fang and S. Farry and D. Fazzini and M. Féo and A. Fernandez Prieto and J. M. Fernandez-tenllado Arribas and A. D. Fernez and F. Ferrari and L. Ferreira Lopes and F. Ferreira Rodrigues and S. Ferreres Sole and M. Ferrillo and M. Ferro-Luzzi and S. Filippov and R. A. Fini and M. Fiorini and M. Firlej and K. M. Fischer and D. Fitzgerald and C. Fitzpatrick and T. Fiutowski and F. Fleuret and M. Fontana and F. Fontanelli and R. Forty and V. Franco Lima and M. Franco Sevilla and M. Frank and E. Franzoso and G. Frau and C. Frei and D. A. Friday and J. Fu and Q. Fuehring and W. Funk and E. Gabriel and T. Gaintseva and A. Gallas Torreira and D. Galli and S. Gambetta and Y. Gan and M. Gandelman and P. Gandini and Y. Gao and M. Garau and L. M. Garcia Martin and P. Garcia Moreno and J. García Pardiñas and B. Garcia Plana and F. A. Garcia Rosales and L. Garrido and C. Gaspar and R. E. Geertsema and D. Gerick and L. L. Gerken and E. Gersabeck and M. Gersabeck and T. Gershon and D. Gerstel and Ph. Ghez and V. Gibson and H. K. Giemza and M. Giovannetti and A. Gioventù and P. Gironella Gironell and L. Giubega and C. Giugliano and K. Gizdov and E. L. Gkougkousis and V. V. Gligorov and C. Göbel and E. Golobardes and D. Golubkov and A. Golutvin and A. Gomes and S. Gomez Fernandez and F. Goncalves Abrantes and M. Goncerz and G. Gong and P. Gorbounov and I. V. Gorelov and C. Gotti and E. Govorkova and J. P. Grabowski and T. Grammatico and L. A. Granado Cardoso and E. Graugés and E. Graverini and G. Graziani and A. Grecu and L. M. Greeven and P. Griffith and L. Grillo and S. Gromov and B. R. Gruberg Cazon and C. Gu and M. Guarise and P. A. Günther and E. Gushchin and A. Guth and Y. Guz and T. Gys and T. Hadavizadeh and G. Haefeli and C. Haen and J. Haimberger and T. Halewood-leagas and P. M. Hamilton and J. P. Hammerich and Q. Han and X. Han and T. H. Hancock and S. Hansmann-Menzemer and N. Harnew and T. Harrison and C. Hasse and M. Hatch and J. He and M. Hecker and K. Heijhoff and K. Heinicke and A. M. Hennequin and K. Hennessy and L. Henry and J. Heuel and A. Hicheur and D. Hill and M. Hilton and S. E. Hollitt and J. Hu and J. Hu and W. Hu and W. Huang and X. Huang and W. Hulsbergen and R. J. Hunter and M. Hushchyn and D. Hutchcroft and D. Hynds and P. Ibis and M. Idzik and D. Ilin and P. Ilten and A. Inglessi and A. Ishteev and K. Ivshin and R. Jacobsson and S. Jakobsen and E. Jans and B. K. Jashal and A. Jawahery and V. Jevtic and M. Jezabek and F. Jiang and M. John and D. Johnson and C. R. Jones and T. P. Jones and B. Jost and N. Jurik and S. Kandybei and Y. Kang and M. Karacson and M. Karpov and F. Keizer and M. Kenzie and T. Ketel and B. Khanji and A. Kharisova and S. Kholodenko and T. Kirn and V. S. Kirsebom and O. Kitouni and S. Klaver and K. Klimaszewski and S. Koliiev and A. Kondybayeva and A. Konoplyannikov and P. Kopciewicz and R. Kopecna and P. Koppenburg and M. Korolev and I. Kostiuk and O. Kot and S. Kotriakhova and P. Kravchenko and L. Kravchuk and R. D. Krawczyk and M. Kreps and F. Kress and S. Kretzschmar and P. Krokovny and W. Krupa and W. Krzemien and W. Kucewicz and M. Kucharczyk and V. Kudryavtsev and H. S. Kuindersma and G. J. Kunde and T. Kvaratskheliya and D. Lacarrere and G. Lafferty and A. Lai and A. Lampis and D. Lancierini and J. J. Lane and R. Lane and G. Lanfranchi and C. Langenbruch and J. Langer and O. Lantwin and T. Latham and F. Lazzari and R. Le Gac and S. H. Lee and R. Lefèvre and A. Leflat and S. Legotin and O. Leroy and T. Lesiak and B. Leverington and H. Li and L. Li and P. Li and S. Li and Y. Li and Y. Li and Z. Li and X. Liang and T. Lin and R. Lindner and V. Lisovskyi and R. Litvinov and G. Liu and H. Liu and S. Liu and X. Liu and A. Loi and J. Lomba Castro and I. Longstaff and J. H. Lopes and G. H. Lovell and Y. Lu and D. Lucchesi and S. Luchuk and M. Lucio Martinez and V. Lukashenko and Y. Luo and A. Lupato and E. Luppi and O. Lupton and A. Lusiani and X. Lyu and L. Ma and R. Ma and S. Maccolini and F. Machefert and F. Maciuc and V. Macko and P. Mackowiak and S. Maddrell-Mander and O. Madejczyk and L. R. Madhan Mohan and O. Maev and A. Maevskiy and D. Maisuzenko and M. W. Majewski and J. J. Malczewski and S. Malde and B. Malecki and A. Malinin and T. Maltsev and H. Malygina and G. Manca and G. Mancinelli and D. Manuzzi and D. Marangotto and J. Maratas and J. F. Marchand and U. Marconi and S. Mariani and C. Marin Benito and M. Marinangeli and J. Marks and A. M. Marshall and P. J. Marshall and G. Martellotti and L. Martinazzoli and M. Martinelli and D. Martinez Santos and F. Martinez Vidal and A. Massafferri and M. Materok and R. Matev and A. Mathad and Z. Mathe and V. Matiunin and C. Matteuzzi and K. R. Mattioli and A. Mauri and E. Maurice and J. Mauricio and M. Mazurek and M. McCann and L. Mcconnell and T. H. Mcgrath and A. McNab and R. McNulty and J. V. Mead and B. Meadows and C. Meaux and G. Meier and N. Meinert and D. Melnychuk and S. Meloni and M. Merk and A. Merli and L. Meyer Garcia and M. Mikhasenko and D. A. Milanes and E. Millard and M. Milovanovic and M. -N. Minard and A. Minotti and L. Minzoni and S. E. Mitchell and B. Mitreska and D. S. Mitzel and A. Mödden and R. A. Mohammed and R. D. Moise and T. Mombächer and I. A. Monroy and S. Monteil and M. Morandin and G. Morello and M. J. Morello and J. Moron and A. B. Morris and A. G. Morris and R. Mountain and H. Mu and F. Muheim and M. Mulder and D. Müller and K. Müller and C. H. Murphy and D. Murray and P. Muzzetto and P. Naik and T. Nakada and R. Nandakumar and T. Nanut and I. Nasteva and M. Needham and I. Neri and N. Neri and S. Neubert and N. Neufeld and R. Newcombe and T. D. Nguyen and C. Nguyen-Mau and E. M. Niel and S. Nieswand and N. Nikitin and N. S. Nolte and C. Nunez and A. Oblakowska-Mucha and V. Obraztsov and D. P. O'Hanlon and R. Oldeman and M. E. Olivares and C. J. G. Onderwater and A. Ossowska and J. M. Otalora Goicochea and T. Ovsiannikova and P. Owen and A. Oyanguren and B. Pagare and P. R. Pais and T. Pajero and A. Palano and M. Palutan and Y. Pan and G. Panshin and A. Papanestis and M. Pappagallo and L. L. Pappalardo and C. Pappenheimer and W. Parker and C. Parkes and C. J. Parkinson and B. Passalacqua and G. Passaleva and A. Pastore and M. Patel and C. Patrignani and C. J. Pawley and A. Pearce and A. Pellegrino and M. Pepe Altarelli and S. Perazzini and D. Pereima and P. Perret and M. Petric and K. Petridis and A. Petrolini and A. Petrov and S. Petrucci and M. Petruzzo and T. T. H. Pham and A. Philippov and L. Pica and M. Piccini and B. Pietrzyk and G. Pietrzyk and M. Pili and D. Pinci and F. Pisani and Resmi P. K and V. Placinta and J. Plews and M. Plo Casasus and F. Polci and M. Poli Lener and M. Poliakova and A. Poluektov and N. Polukhina and I. Polyakov and E. Polycarpo and G. J. Pomery and S. Ponce and D. Popov and S. Popov and S. Poslavskii and K. Prasanth and L. Promberger and C. Prouve and V. Pugatch and H. Pullen and G. Punzi and W. Qian and J. Qin and R. Quagliani and B. Quintana and N. V. Raab and R. I. Rabadan Trejo and B. Rachwal and J. H. Rademacker and M. Rama and M. Ramos Pernas and M. S. Rangel and F. Ratnikov and G. Raven and M. Reboud and F. Redi and F. Reiss and C. Remon Alepuz and Z. Ren and V. Renaudin and R. Ribatti and S. Ricciardi and K. Rinnert and P. Robbe and G. Robertson and A. B. Rodrigues and E. Rodrigues and J. A. Rodriguez Lopez and A. Rollings and P. Roloff and V. Romanovskiy and M. Romero Lamas and A. Romero Vidal and J. D. Roth and M. Rotondo and M. S. Rudolph and T. Ruf and J. Ruiz Vidal and A. Ryzhikov and J. Ryzka and J. J. Saborido Silva and N. Sagidova and N. Sahoo and B. Saitta and M. Salomoni and D. Sanchez Gonzalo and C. Sanchez Gras and R. Santacesaria and C. Santamarina Rios and M. Santimaria and E. Santovetti and D. Saranin and G. Sarpis and M. Sarpis and A. Sarti and C. Satriano and A. Satta and M. Saur and D. Savrina and H. Sazak and L. G. Scantlebury Smead and S. Schael and M. Schellenberg and M. Schiller and H. Schindler and M. Schmelling and B. Schmidt and O. Schneider and A. Schopper and M. Schubiger and S. Schulte and M. H. Schune and R. Schwemmer and B. Sciascia and S. Sellam and A. Semennikov and M. Senghi Soares and A. Sergi and N. Serra and L. Sestini and A. Seuthe and P. Seyfert and Y. Shang and D. M. Shangase and M. Shapkin and I. Shchemerov and L. Shchutska and T. Shears and L. Shekhtman and Z. Shen and V. Shevchenko and E. B. Shields and E. Shmanin and J. D. Shupperd and B. G. Siddi and R. Silva Coutinho and G. Simi and S. Simone and N. Skidmore and T. Skwarnicki and M. W. Slater and I. Slazyk and J. C. Smallwood and J. G. Smeaton and A. Smetkina and E. Smith and M. Smith and A. Snoch and M. Soares and L. Soares Lavra and M. D. Sokoloff and F. J. P. Soler and A. Solovev and I. Solovyev and F. L. Souza De Almeida and B. Souza De Paula and B. Spaan and E. Spadaro Norella and P. Spradlin and F. Stagni and M. Stahl and S. Stahl and P. Stefko and O. Steinkamp and O. Stenyakin and H. Stevens and S. Stone and M. E. Stramaglia and M. Straticiuc and D. Strekalina and F. Suljik and J. Sun and L. Sun and Y. Sun and P. Svihra and P. N. Swallow and K. Swientek and A. Szabelski and T. Szumlak and M. Szymanski and S. Taneja and F. Teubert and E. Thomas and K. A. Thomson and V. Tisserand and S. T'Jampens and M. Tobin and L. Tomassetti and D. Torres Machado and D. Y. Tou and M. T. Tran and E. Trifonova and C. Trippl and G. Tuci and A. Tully and N. Tuning and A. Ukleja and D. J. Unverzagt and E. Ursov and A. Usachov and A. Ustyuzhanin and U. Uwer and A. Vagner and V. Vagnoni and A. Valassi and G. Valenti and N. Valls Canudas and M. van Beuzekom and M. Van Dijk and E. van Herwijnen and C. B. Van Hulse and M. van Veghel and R. Vazquez Gomez and P. Vazquez Regueiro and C. Vázquez Sierra and S. Vecchi and J. J. Velthuis and M. Veltri and A. Venkateswaran and M. Veronesi and M. Vesterinen and D. Vieira and M. Vieites Diaz and H. Viemann and X. Vilasis-Cardona and E. Vilella Figueras and P. Vincent and D. Vom Bruch and A. Vorobyev and V. Vorobyev and N. Voropaev and R. Waldi and J. Walsh and C. Wang and J. Wang and J. Wang and J. Wang and J. Wang and M. Wang and R. Wang and Y. Wang and Z. Wang and Z. Wang and H. M. Wark and N. K. Watson and S. G. Weber and D. Websdale and C. Weisser and B. D. C. Westhenry and D. J. White and M. Whitehead and D. Wiedner and G. Wilkinson and M. Wilkinson and I. Williams and M. Williams and M. R. J. Williams and F. F. Wilson and W. Wislicki and M. Witek and L. Witola and G. Wormser and S. A. Wotton and H. Wu and K. Wyllie and Z. Xiang and D. Xiao and Y. Xie and A. Xu and J. Xu and L. Xu and M. Xu and Q. Xu and Z. Xu and Z. Xu and D. Yang and S. Yang and Y. Yang and Z. Yang and Z. Yang and Y. Yao and L. E. Yeomans and H. Yin and J. Yu and X. Yuan and O. Yushchenko and E. Zaffaroni and M. Zavertyaev and M. Zdybal and O. Zenaiev and M. Zeng and D. Zhang and L. Zhang and S. Zhang and Y. Zhang and Y. Zhang and A. Zhelezov and Y. Zheng and X. Zhou and Y. Zhou and X. Zhu and Z. Zhu and V. Zhukov and J. B. Zonneveld and Q. Zou and S. Zucchelli and D. Zuliani and G. Zunica},
      year={2021},
      eprint={2103.11769},
      archivePrefix={arXiv},
      primaryClass={hep-ex}
}

@article{BERNSTEIN201327,
title = {Charged lepton flavor violation: An experimenter’s guide},
journal = {Physics Reports},
volume = {532},
number = {2},
pages = {27-64},
year = {2013},
note = {Charged Lepton Flavor Violation: An Experimenter's Guide},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2013.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0370157313002688},
author = {R.H. Bernstein and Peter S. Cooper},
keywords = {Electron, Muon, Tau, Flavor},
abstract = {Charged lepton flavor violation (CLFV) is a clear signal of new physics; it directly addresses the physics of flavor and of generations. The search for CLFV has continued from the early 1940s, when the muon was identified as a separate particle, until today. Certainly in the LHC era the motivations for continued searches are clear and have been covered in many reviews. This review is focused on the experimental history with a view toward how these searches might progress. We examine the status of searches for charged lepton flavor violation in the muon, tau, and other channels, and then examine the prospects for new efforts over the next decade. Finally, we examine what paths might be taken after the conclusion of upcoming experiments and what facilities might be required.}
}

@article{Baldini2018,
author={Baldini, A. M.
and Baracchini, E.
and Bemporad, C.
and Berg, F.
and Biasotti, M.
and Boca, G.
and Cattaneo, P. W.
and Cavoto, G.
and Cei, F.
and Chiappini, M.
and Chiarello, G.
and Chiri, C.
and Cocciolo, G.
and Corvaglia, A.
and de Bari, A.
and De Gerone, M.
and D'Onofrio, A.
and Francesconi, M.
and Fujii, Y.
and Galli, L.
and Gatti, F.
and Grancagnolo, F.
and Grassi, M.
and Grigoriev, D. N.
and Hildebrandt, M.
and Hodge, Z.
and Ieki, K.
and Ignatov, F.
and Iwai, R.
and Iwamoto, T.
and Kaneko, D.
and Kasami, K.
and Kettle, P.-R.
and Khazin, B. I.
and Khomutov, N.
and Korenchenko, A.
and Kravchuk, N.
and Libeiro, T.
and Maki, M.
and Matsuzawa, N.
and Mihara, S.
and Milgie, M.
and Molzon, W.
and Mori, Toshinori
and Morsani, F.
and Mtchedilishvili, A.
and Nakao, M.
and Nakaura, S.
and Nicol{\`o}, D.
and Nishiguchi, H.
and Nishimura, M.
and Ogawa, S.
and Ootani, W.
and Panareo, M.
and Papa, A.
and Pepino, A.
and Piredda, G.
and Popov, A.
and Raffaelli, F.
and Renga, F.
and Ripiccini, E.
and Ritt, S.
and Rossella, M.
and Rutar, G.
and Sawada, R.
and Signorelli, G.
and Simonetta, M.
and Tassielli, G. F.
and Uchiyama, Y.
and Usami, M.
and Venturini, M.
and Voena, C.
and Yoshida, K.
and Yudin, Yu. V.
and Zhang, Y.},
title={The design of the MEG II experiment},
journal={The European Physical Journal C},
year={2018},
month={May},
day={16},
volume={78},
number={5},
pages={380},
abstract={The MEG experiment, designed to search for the {\$}{\$}{\{}{\backslash}mu ^+ {\backslash}rightarrow {\backslash}hbox {\{}e{\}}^+ {\backslash}gamma {\}}{\$}{\$}decay, completed data-taking in 2013 reaching a sensitivity level of {\$}{\$}{\{}5.3{\backslash}times 10^{\{}-13{\}}{\}}{\$}{\$}for the branching ratio. In order to increase the sensitivity reach of the experiment by an order of magnitude to the level of {\$}{\$}6{\backslash}times 10^{\{}-14{\}}{\$}{\$}, a total upgrade, involving substantial changes to the experiment, has been undertaken, known as MEG II. We present both the motivation for the upgrade and a detailed overview of the design of the experiment and of the expected detector performance.},
issn={1434-6052},
doi={10.1140/epjc/s10052-018-5845-6},
url={https://doi.org/10.1140/epjc/s10052-018-5845-6}
}

@article{osti_1172555,
title = {Mu2e Technical Design Report},
author = {Bartoszek, L. and et al.},
abstractNote = {Fermi National Accelerator Laboratory and the Mu2e Collaboration, composed of about 155 scientists and engineers from 28 universities and laboratories around the world, have collaborated to create this technical design for a new facility to study charged lepton flavor violation using the existing Department of Energy investment in the Fermilab accelerator complex. Mu2e proposes to measure the ratio of the rate of the neutrinoless, coherent conversion of muons into electrons in the field of a nucleus, relative to the rate of ordinary muon capture on the nucleus. The conversion process is an example of charged lepton flavor violation (CLFV), a process that has never been observed experimentally. The significant motivation behind the search for muon-to-electron conversion is discussed in Chapter 3.},
doi = {10.2172/1172555},
url = {https://www.osti.gov/biblio/1172555}, journal = {},
place = {United States},
year = {2014},
month = {10}
} 

@article{ARNDT2021165679,
title = {Technical design of the phase I Mu3e experiment},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {1014},
pages = {165679},
year = {2021},
issn = {0168-9002},
doi = {https://doi.org/10.1016/j.nima.2021.165679},
url = {https://www.sciencedirect.com/science/article/pii/S0168900221006641},
author = {K. Arndt and H. Augustin and P. Baesso and N. Berger and F. Berg and C. Betancourt and D. Bortoletto and A. Bravar and K. Briggl and D. {vom Bruch} and A. Buonaura and F. Cadoux and C. Chavez Barajas and H. Chen and K. Clark and P. Cooke and S. Corrodi and A. Damyanova and Y. Demets and S. Dittmeier and P. Eckert and F. Ehrler and D. Fahrni and S. Gagneur and L. Gerritzen and J. Goldstein and D. Gottschalk and C. Grab and R. Gredig and A. Groves and J. Hammerich and U. Hartenstein and U. Hartmann and H. Hayward and A. Herkert and G. Hesketh and S. Hetzel and M. Hildebrandt and Z. Hodge and A. Hofer and Q.H. Huang and S. Hughes and L. Huth and D.M. Immig and T. Jones and M. Jones and H.-C. Kästli and M. Köppel and P.-R. Kettle and M. Kiehn and S. Kilani and H. Klingenmeyer and A. Knecht and A. Knight and B. Kotlinski and A. Kozlinskiy and R. Leys and G. Lockwood and A. Loreti and D. {La Marra} and M. Müller and B. Meier and F. Meier Aeschbacher and A. Meneses and K. Metodiev and A. Mtchedlishvili and S. Muley and Y. Munwes and L.O.S. Noehte and P. Owen and A. Papa and I. Paraskevas and I. Perić and A.-K. Perrevoort and R. Plackett and M. Pohl and S. Ritt and P. Robmann and N. Rompotis and T. Rudzki and G. Rutar and A. Schöning and R. Schimassek and H.-C. Schultz-Coulon and N. Serra and W. Shen and I. Shipsey and S. Shrestha and O. Steinkamp and A. Stoykov and U. Straumann and S. Streuli and K. Stumpf and N. Tata and J. Velthuis and L. Vigani and E. Vilella-Figueras and J. Vossebeld and R. Wallny and A. Wasili and F. Wauters and A. Weber and D. Wiedner and B. Windelband and T. Zhong},
keywords = {Lepton flavour violation, Muon decays, Monolithic pixel detector, Scintillating fibres, Scintillating tiles},
abstract = {The Mu3e experiment aims to find or exclude the lepton flavour violating decay μ→eee at branching fractions above 10−16. A first phase of the experiment using an existing beamline at the Paul Scherrer Institute (PSI) is designed to reach a single event sensitivity of 2⋅10−15. We present an overview of all aspects of the technical design and expected performance of the phase I Mu3e detector. The high rate of up to 108 muon decays per second and the low momenta of the decay electrons and positrons pose a unique set of challenges, which we tackle using an ultra thin tracking detector based on high-voltage monolithic active pixel sensors combined with scintillating fibres and tiles for precise timing measurements.}
}
