\chapter{Fake Monte Carlo data generation}
\begin{markdown}
---

- We are coming from the simulation chapter, so:
- Problem description: 
 - We would like to produce larger amounts of simulation data, e.g. for a mock dataset in preparation for data-taking
 - But we have limited capability to produce MC data with simulation. The main bottleneck is in the pion-production section's hadronic interactions. Is there a faster way?
  - The topic of this chapter is to consider the application of machine-learning methods in the fabrication of fake Monte Carlo data.


- Creation of dataset from Monte Carlo simulation
  - Hit pattern characterisation
  - Reconstructible vs noise-like classification
- GAN theory [Goodfellow, WGAN, WGAN-GP]
 - Pre-processing
- GAN design, basic architecture, softmax for classes [WGAN-GP]
- GAN extensions, embedding? analogy with text?

---
\end{markdown}

% Need to wrap the start of this chapter into a compelling narrative:
% 1. MC allows us to see possible patterns in detector sys
%  e.g., in CDC, high-mom tracks produce structured hits whereas low-mom make small 
%  localised hit clusters --> Draw figures
% 2. For an exercise such as a mock-data challenge (explain term),
%  traditional MC is not efficient enough to produce the dataset.
%  + Think about: why do we want mock data? Why is the standard sensitivity estimation
%    process not sufficient?
%    - Perhaps the extrapolation factor. Considering efficiencies on a limited sample cannot
%       effectively be representative of the real situation.
%    - The sheer sample size makes the exercise worth it? Test limits of the data processing
%       chain and increase preparedness for real situation.
% 3. Now consider the problem of producing artificial / fake / imitation / 
%  fictitious / fabricated data.
%  + Consider alternatives
%  + Introduce GAN for detector-level data fabrication.

As discussed in the previous chapter, Monte Carlo simulation allows us to draw a realistic picture of what will happen in our experimental setup once it is running. Most importantly, it provides an estimate of the kinds of patterns we can expect to observe in the detector system. 

When performing a full simulation of COMET Phase-I, most of the activity takes place in the initial collision between the proton beam and the graphite target. The many hadronic interactions caused by the high-energy protons represent \SI{99.7}{\percent} of the computational cost of the Monte Carlo simulation.
In contrast to that, because of the large distance between the pion-production section and the detector area, only one muon will come at rest in the stopping target for around every \num{2000} proton-on-target (POT) collisions. Hence, producing Monte Carlo data in the detector system by simulating POT events one by one is a very inefficient process. 
Statistically, any one POT collision only has a \SI{0.12}{\percent} probability of producing observable information in the detector system (i.e.\ hits), such that the entirety of the computing time spent in the simulating the other \SI{99.88}{\percent} of collisions is physically irrelevant.

% Calculation of above number on MC5
% We can RooTrackerTree->Draw("StdHepN", "StdHepN>0") to get the number of events with a downstream track, but to get the number of events with a hit, it's a bit more involved...
% We need to use the downstream root files
% MC5A01: StdHepN>0 --> 27267993 / 990678400 ~= 2.75% in all RT files
% ---> 2.75% of all POT events lead to >=1 particle entering the detector region
% Repeat on MC5A02 RT files to get stopped muons per POT
% MC5A02: StdHepN>0 && StdHepPdg==13 --> 486051 / 990678399 ~= 0.049%
% ---> 0.049% of all POT events lead to >=1 stopped muon
% /sps/mc5a02/count_events_with_cdc_hits.C --> 5939 / 4980700 ~= 0.12% in first root file
% /sps/mc5a02/count_events_with_cdc_hits.C --> 1173800 / 990678399 ~= 0.1185% in all dataset
    % Watch job 20862516 output for answer
% ---> 0.12% of all POT events lead to >=1 hits in the CDC
% CORRECTION: we counted events where there are hits even if none of them have edep>0
% --> Only count events where at least one edep>0 hit occurs:
% /sps/mc5a02/count_events_with_cdc_hits_edep.C --> 260 / 4980700 ~= 5.22e-5 in first root file
% in first 10: 2611 / 49468900 ~= 5.278e-5
% extrapolate to all = 52289 / 990678399

On average, simulating a single POT event in the upstream region requires \SI{2.73}{\second}. A single proton bunch contains \num{16000000} protons, which, if simulated linearly would take 500 days of computation. The MC5 production ran for around two weeks on 2000 concurrent machines in order to fully simulate 62 bunches, around $1\times10^9$ protons. During the data-taking period for COMET Phase-I, $2\times10^{12}$ bunches, or $1.6\times 10^{19}$ protons, are expected to hit the target. Simulating a similar amount of MC data is simply impossible with the same methods and infrastructure used to produce MC5.

% Why would we want to do that though? Does anyone else do it? Asked Joe
 % Joe says: they only really care about signal MC, MC of just background seems pointless to them...
% Because they weigh events: they have a rough idea, from event generators, of how likely any event is. Our approach is more brute-forcey. 

% Before talking about why it's impossible, we need to bring up the fact that we 
% WANT to get that much data. We need to motivate this...
% Mock-data
% In COMET, one event could make the difference between absence of signal and CLFV discovery.
% Because of the beam transport, it is hard to estimate the kinds background that can enter the detector system.
% -> MC is heavily used to constrain or estimate the rates of various background processes

In order to alleviate this issue, multiple solutions can be considered. One that is currently used in \texttt{SimG4} is to split the simulation into spatial regions, as was described in section~\ref{sec:mc5}. At the boundary between the two regions, particle kinematics (position and momentum) are recorded to a \texttt{oaRooTracker} file, to be later tracked inside the region of interest which contains the detector. Because of the stochastic nature of MC simulations, it is possible to modify the random seed used to simulate any given event. Hence from the same set of recorded particle kinematics, a multitude of outcomes can potentially be realised by re-seeding the simulation, thus artificially increasing the MC statistics.

Another possibility is to estimate the position and momentum distribution of particles at the boundary to later sample more events. In ICEDUST, the \texttt{RooTrackerHistogram} functionality allows \texttt{oaRooTracker} files, which contain saved particle kinematics, to be turned into histograms from which events can later be generated.

Alternatively, particles produced in the hadronic interactions which have a small probability of resulting in observable detector hits could also be aggressively culled, at the risk of potentially suppressing important events. To eliminate particles, a straightforward algorithm cutting on position, momentum and particle type might suffice, but a more intelligent classifying algorithm (e.g.\ a decision tree) which uses these features and others could improve the selection efficiency. 

The option considered in this chapter to increase the amount of MC data that can be produced is to partially replace the Monte Carlo simulation by a faster generator of hit data at the detector level. This generator should replicate the micro and macro hit patterns produced by simulated particles in the detector, without relying on a full Monte Carlo approach (i.e.\ particle-by-particle tracking).

In this chapter, I will discuss my implementation of a Generative Adversarial Networks-based solution to the problem of fast realistic hit generation in the CDC. This restricted application ultimately strives toward a CyDet-wide event sampling algorithm whose lower computational cost compared to traditional Monte Carlo simulation would enable the production of larger datasets, on a scale approaching that of the Phase-I data-taking run.

\section{Data selection} % Name?
% Describe the procedure to go from MC data to fake datasets, can use some math abstraction for the GAN, indicating that it could be something else entirely, e.g. G(z) = fake hits, F(x_i, p_i, PID_i) = real hits, distribution(G(z)) ~ distribution(F(.))
If our aim is to build a generative model to mimic the patterns of hits in the detector, it is necessary to keep in mind that these hits are physical energy deposits created by simulating energy loss processes of particles inside the active detector volume. The true process of producing hits thus obeys very strict rules stemming from the physical laws in play. The model we consider here is purely data-driven in that we do not enforce any awareness of physics onto it. Although this choice makes the potential speed-up significant, it also means that we lose much of our control over the content of produced data and rely on the accuracy of the model to yield physically realistic hits.

In COMET Phase-I, events of particular interest come in the form of extended tracks in the CDC, whose accurate reconstruction (in both momentum and position) is paramount to the identification of a conversion signal. These tracks typically produce hits in very specific patterns, dictated by the particle's exact trajectory and energy loss in the gas. We expect a data-driven generative model to struggle in replicating the exact signature of such tracks, hence we decided early on to restrict the scope of the model to events that lack any physics-rich features. 
% ^ OK


% Consequences: we have to combine MC and GAN data
% ---> means we have to find a way to produce MC selectively
% ---> and then mix hits in realistic proportions to get mock data.

%In addition to the rigid constraints involved in producing tracks, it is preferable from the point of view of the reconstruction algorithms to be able to query their true provenance. A physics-agnostic model is by definition unable to provide that kind of truth information

Not all MC data can be replaced by an alternative algorithm. Monte Carlo simulation has the particularity of providing full truth information about every hit in the detector system, including the type and whereabouts of the particles that produced them.
Truth information is typically useful when a series of hits is susceptible to be reconstructed into a track, whose true provenance will be compared to the guess from the reconstruction algorithm. % Think about this. Is this true? What's the point of reconstruction? Should we consider a hypothetical scenario where we have a dataset with an unknown number of signal tracks and we ask someone to find the needles in the haystack?
% Then why isn't our classification simply signal tracks versus all other tracks?

% Characterisation of tracks in the CDC: high-momentum vs low-momentum
Thankfully, not all hits in the detector require associated truth information, and in fact the majority of hits in the CDC appear like background noise whose origin is irrelevant to any reconstruction procedure. % The question is: DEFINE EXACTLY WHAT KIND OF DATA NEEDS TRUTH
% Illustrate with figure ?
%To illustrate this idea, Fig.~\ref{fig:noise_vs_recon} showcases hits produced by two types of particles.

To define our problem, let us consider the situation where a large quantity of artificial data must be produced but traditional simulation is simply too inefficient to reach the desired sample size.

% ^^^^ Review up to ^OK

\subsection{Hit categorisation}
% This is what hits from p>50MeV tracks look like, and this is what hits from p<50MeV tracks look like.
% From the observation that the former tends to have reconstructible hit patterns while the latter contains single hits and localised patterns, we categorise MC events according to whether they contain a p>50MeV track or not.
% We then use hits from non-reconstructible events as the training data for our generative model whose goal is to learn the patterns and generate original samples of hits that we can use as a background onto which we will overlay reconstructible MC-simulated events.
% The result is the offloading of 99% of the simulation to the generative model which is highly efficient. Since the information generated is 
Fig.~\ref{fig:cdc_rconst_vs_noise} outlines the difference in structure between hits produced by $p\leq\SI{50}{\MeV/\clight}$ tracks~(\ref{fig:cdc_rconst_vs_noise:low}) and hits produced by $p>\SI{50}{\MeV/\clight}$ tracks~(\ref{fig:cdc_rconst_vs_noise:high}) in the CDC. Generally, this loose momentum threshold separates what the detector recognises as prolonged circular tracks from more localised clusters of hits.

\begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        %\includegraphics[width=\textwidth]{example-image}
        \hl{TODO}
        \caption{$p\leq\SI{50}{\MeV/\clight}$}
        \label{fig:cdc_rconst_vs_noise:low}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\textwidth}
        \centering
        %\includegraphics[width=\textwidth]{example-image}
        \hl{TODO}
        \caption{$p > \SI{50}{\MeV/\clight}$}
        \label{fig:cdc_rconst_vs_noise:high}
    \end{subfigure}
    \caption{Comparison of hit patterns in the CDC produced by tracks of different momenta.}
    \label{fig:cdc_rconst_vs_noise}
\end{figure}

This observation allows us to define \emph{reconstructible} events as those that induce one or more such high-momentum tracks, while other events can be seen as merely sources of \emph{noise} in the detector. This noise will be the target of our generative model. Statistically, more than 97\% of hit-inducing events contain only such hits, so offloading that part of the simulation to an efficient generator could greatly improve the total amount of data that can be produced.

% MC5A02 calculation of noise-like vs reconstructible events/hits: 
% Total events: 990678399
%  Hit classification: 
%  2036 unique events (rate 2e-6) with reconstructible hits (63648 hits)
%  49101 unique events (rate 5e-5) with noiselike hits (634266 hits)




\section{Generative Adversarial Networks}
Generative Adversarial Networks (GANs) are a class of generative models which can learn the underlying distribution of a dataset in order to synthesise new samples~\cite{goodfellow_generative_2014}. Training a GAN requires two neural networks to compete in a zero-sum game where one network generates samples and the other tries to discriminate between real and generated data.

As the classical example, let us consider the case where both the discriminator and the generator are multi-layer perceptrons denoted as $D$ and $G$, respectively. $D$ is built as a binary classifier whose goal is to tell apart real samples (from a \emph{training} dataset) from fake (generated) ones. Given a sample $\mathbf{x}$, it outputs a score between 0 and 1, i.e.\ $D(\mathbf{x}) \in [0, 1]$. Similar to a classification task, we define the loss function of $D$ as the cross entropy between its prediction and the true label (conventionally, 0 for a fake sample and 1 for a real sample):
\begin{equation}\label{eq:D_loss}
    \mathcal{L}_D = 
    -\mathbb{E}_{\mathbf{x} \sim p} [ \log D(\mathbf{x}) ] -
    \mathbb{E}_{\tilde{\mathbf{x}} \sim g} [ \log( 1 - D(\tilde{\mathbf{x}}) )],
\end{equation}
where $\mathbb{E}$ denotes the expected value or mean, $p$ is the distribution of real samples and $g$ is the distribution of samples generated by $G$.
Intuitively, minimising this function means that the discriminator will assign a high score to samples drawn from the training dataset and a low score to generated samples.


The generator produces fake samples by mapping vectors from a latent space into data space. Latent-space vectors $\mathbf{z}$ are sampled according to a prior $p_\mathbf{z}(\mathbf{z})$, typically a multivariate normal distribution for simplicity and speed. The objective of $G$ is to generate samples $\tilde{\mathbf{x}} = G(\mathbf{z} \sim p_\mathbf{z})$ such that $D(\tilde{\mathbf{x}}) \rightarrow 1$. In other words, the generator aims to maximise the second term in Eq.~\ref{eq:D_loss}, and its loss function is
\begin{align*}
    \mathcal{L}_G &=     
    \mathbb{E}_{\tilde{\mathbf{x}} \sim g} [ \log( 1 - D(\tilde{\mathbf{x}}) )]\\
    &= \mathbb{E}_{\mathbf{z} \sim p_\mathbf{z}} [ \log( 1 - D(G(\mathbf{z}) )].
\end{align*}

Combining these minimisation tasks, we obtain the mathematical formulation of the adversarial training objective:
\begin{equation}\label{eq:GAN}
    \min_G \max_D \quad
    \mathbb{E}_{\mathbf{x} \sim p} [ \log D(\mathbf{x}) ] +
    \mathbb{E}_{\mathbf{z} \sim p_\mathbf{z}} [ \log( 1 - D(G(\mathbf{z}) )].
\end{equation}
Using neural networks for $D$ and $G$ allows the above minimax solution to be approximated via backpropagation and stochastic gradient descent. At every iteration, both networks evaluate the gradient of their respective loss function with respect to their internal weights. Every weight is then adjusted toward the direction of largest decrease in the loss.


\subsection{Wasserstein GAN}
% WGAN, WGAN with Gradient Penalty
The original formulation of GAN is notoriously difficult to train due to either non-convergence, instability or mode collapse\footnote{Mode collapse is the situation where $G$ maps every point in the latent space onto the same output, leading to low diversity in the generated samples.}. 
Training a GAN model is highly sensitive to the choice of hyperparameters: learning rate, optimisation algorithm, network architecture. 

The Wasserstein GAN (WGAN) formulation is an attempt to address the stability issues of the original GAN~\cite{arjovsky2017wasserstein}. The authors argue that solving Eq.~\ref{eq:GAN}, which implicitly minimises the Jensen-Shannon divergence between $p$ and $g$, leads to vanishing gradients when the discriminator is too powerful, and thus to unstable training.
Instead, they propose to minimise the Wasserstein-1 distance between $p$ and $g$ because of its superior continuity and differentiability properties. They go on to show that if the discriminator is 1-Lipschitz continuous, then the adversarial training problem can be formulated as:
\begin{equation}
    \min_G \max_D \quad 
        \mathbb{E}_{\mathbf{x} \sim p} \left[ D(\mathbf{x}) \right] - 
        \mathbb{E}_{\mathbf{z} \sim p_\mathbf{z}} \left[ D(G(\mathbf{z})) \right].
\end{equation}

In practice, aside from a change to the loss functions, this method requires that the discriminator be replaced by a ``critic'', so-called because its output is not bounded to $[0, 1]$ and can be better interpreted as a score. In order for $D$ to satisfy the Lipschitz constraint, the authors propose to restrict the magnitude of its weights to a small range, e.g. $[-0.01, 0.01]$.
%explicitly stating that this is not an optimal solution. 

In addition to demonstrating the superior training stability of this WGAN method, it was observed empirically that this formulation does not lead to mode collapse and improves the robustness of the GAN with respect to changes in the network architectures.

% WGAN-GP
The same year, another method to enforce the Lipschitz constraint on $D$ was discussed in~\cite{NIPS2017_892c3b1c}, which outlines the shortcomings of weight clipping and instead proposes to constrain the discriminator's gradient.
By definition, a function is 1-Lipschitz if and only if its gradient has norm at most 1 everywhere. Since this is difficult to achieve in practice, the authors suggest a soft constraint on the gradient norm of $D$ using an explicit term in its loss function:
\begin{equation}\label{eq:WGAN-GP}
    \mathcal{L}_D = 
        -\mathbb{E}_{\mathbf{x} \sim p} \left[ D(\mathbf{x}) \right] +
        \mathbb{E}_{\mathbf{z} \sim p_\mathbf{z}} \left[ D(G(\mathbf{z})) \right] +
        \underbrace{\lambda\ \mathbb{E}_{\hat{\mathbf{x}} \sim p_{\hat{\mathbf{x}}}}
            \left[ \big( \left\Vert \nabla_{\hat{\mathbf{x}}}\ D(\hat{\mathbf{x}}) \right\Vert_2 - 1 \big) ^2 \right]}_\textrm{Gradient penalty},
\end{equation}
where the third term is the gradient penalty (GP) added to the WGAN loss, and $\lambda$ is the gradient penalty constant, a new hyperparameter. In this term, samples $\hat{\mathbf{x}}$ are drawn from $p_{\hat{\mathbf{x}}}$ by sampling uniformly along straight lines between pairs of points from $p$ and $g$. 

In our experiments, it was found that the above WGAN-GP loss provides the most stable training procedure for a variety of network architectures. In addition, it has the significant advantage of making the critic loss more interpretable and overfitting noticeable: as the authors demonstrate, the critic loss should converge to a maximum value over training iterations, while in the case of overfit, the critic losses evaluated on training samples and test samples diverge during training.

% Examples of usage in HEP (see zotero)
\subsection{GANs in High Energy Physics}
In high-energy physics, GANs have been proposed in a variety of experiments to supplement traditional Monte Carlo simulation. Usage of GANs in HEP typically falls in one of two categories: event generation, and generation of hit data at the detector level.

To speed up event generation for collider experiments, GANs have been used to simulate final-state kinematics for $Z$ and top production and decay~\cite{butter_how_2019, otten_event_2019}. 

In the context of the SHiP experiment, a GAN was used to generate muons produced in the \SI{400}{\GeV/\clight} proton beam collision with a fixed target~\cite{ahdida_fast_2019}.

In experiments where hadronic jets are a common sight in the detector system, such as ATLAS and CMS, deep convolutional GANs were proposed as an alternative to MC simulations to generate \emph{jet images}, a 2D representation of the energy deposition patterns from jets~\cite{deOliveira2017b}. 

Similarly, for generating realistic electromagnetic shower signatures inside a 3D calorimeter, the {\sc CaloGAN} model was developed~\cite{paganini_calogan_2018}. As a means to obtain more physically realistic samples, the generator is conditioned to produce appropriate showers given the energy of the inbound particle, such that conservation of total energy may be implicitly learned by the model along with the distribution of shower patterns.

A GAN model was trained to generate dimuon final states from $Z$ decay, toward the production of large analysis-specific datasets for the High-Luminosity LHC~\cite{hashemi2019lhc}.

WGAN-GP for cosmic ray-induced showers in water Cherenkov \cite{Erdmann2018}.

WGAN-GP for calorimeter~\cite{Erdmann2019}.




% How do we plan on using this to sample CDC hits? Big brain time
\section{The CDC hit generator}
\hl{Intro bad}
In our case, we chose to make use of a GAN model to synthesise hits in the CDC.
% Motivate: we bypass the bottleneck and produce data where it is most needed
Hits from non-reconstructible events are selected and extracted to form the training dataset. Each hit is defined by four features: energy deposit, distance of closest approach between the track and wire, timing and position. We represent position by an integer wire index, ranging from 0 to 4986, which receives a different treatment to the other three continuous features.

Training the GAN to generate individual hits is sufficient to model the underlying distributions of the four features, but the generated hits lack coherence. In the training dataset, when multiple hits are produced by a single track, they often occur next to each other in time and space. The GAN model must be allowed to see and process the relationship between consecutive hits for the generated hits to resemble simulated ones. Hence instead of having the model process single hits, it makes sense to feed it sequences of consecutive hits such that temporal information can be processed as well. The architecture of $G$ and $D$ should be allowed to accommodate for this extra dimension.



\subsection{Network architectures}
Both the generator and discriminator are built using one-dimensional convolutional layers.
The convolution kernels move along the temporal dimension, as illustrated in Fig.~\ref{fig:temporal_conv}. 
In this context, the convolution operation is defined as follows:
\begin{equation}
    y_{i} = \sum_{j=1}^{C_\mathrm{in}} \sum_{k=-(K-1)/2}^{(K-1)/2} w_k\ x_{i+k,j} + b,
\end{equation}
where $i$ is the sequence index, $C_\mathrm{in}$ is the number of features\footnote{Also called the number of channels or feature maps.} in the input, $K$ is the kernel size, $w_k$ is the $k$-th element of the kernel, $x$ is the input sequence and $b$ is the bias. The kernel elements $w$ and bias $b$ are learned parameters of the neural network. 
Convolutional layers typically have multiple output channels, each corresponding to a kernel and bias which extract different features from the input.

The fact that the convolution kernel is moved along the sequence means that the network effectively considers the features of multiple consecutive hits at a time. The effective receptive field of the networks, in terms of consecutive hits, increases with the kernel size of the convolutional layers as well as the total number of layers (i.e.\ depth).
\begin{figure}
    \centering
    %\includegraphics{}
    \hl{TODO}
    \caption{Temporal (1D) convolution}
    \label{fig:temporal_conv}
\end{figure}

The discriminator takes as input a sequence of hits of length $L$. Each hit is described by four features: energy deposit, time, distance of closest approach and wire index. The first convolutional layer computes feature maps by sliding its kernels over the hit sequence. A non-linear activation function is applied to the feature maps before passing them on to the next layer. 
In our case, the leaky rectified linear unit, or \texttt{LeakyReLU}~\cite{Maas13rectifiernonlinearities}, is used as the non-linearity.
% ^ Too much detail?

Optionally, convolution kernels can be applied over the sequence with a stride, by moving the kernel by more than one input element to obtain the next output element. This has the effect of producing an output sequence that is shorter than the input sequence. This is often used to allow the network to see the data over increasingly large scales as well as to reduce the number of operations in deeper layers. 

\subsubsection{Residual connections}
\begin{figure}
    \centering
    %\includegraphics{}
    \caption{Residual block.}
    \label{fig:residual_block}
\end{figure}
In order to facilitate gradient flow from the discriminator to the generator and thus speed up training, convolutional layers are promoted to residual blocks~\cite{he2016deep} in both networks. A residual block combines two convolutional layers with a residual connection from the input to the output, allowing the gradients to propagate via two paths, as shown in Fig.~\ref{fig:residual_block}. The output $\mathbf{y}$ of the block can be written as
\begin{equation*}
    \mathbf{y} = \mathcal{F}(\mathbf{x}) + W_s\mathbf{x},
\end{equation*}
where $\mathcal{F}$ represents the convolutions and $W_s$ is a linear dimension-matching operation which brings $\mathbf{x}$ into the same shape as $\mathbf{y}$, if necessary.
    
The critic architecture is shown in full in Fig.~\ref{fig:disc_arch}. It uses \hl{N} residual blocks to extract features from the input hit sequence and outputs a single score following a global pooling operation and a fully-connected linear layer.

\begin{figure}
    \centering
    %\includegraphics{}
    \caption{Architecture of the critic network.}
    \label{fig:disc_arch}
\end{figure}

The generator architecture is shown in Fig.~\ref{fig:gen_arch}.

\begin{figure}
    \centering
    %\includegraphics{}
    \caption{Architecture of the generator network.}
    \label{fig:gen_arch}
\end{figure}

\subsection{Training}

The GAN is trained with the WGAN-GP loss on Nvidia V100 GPUs.
Optimiser: Adam

\subsection{Evaluation}


%%Compared to other methods such as variational auto-encoders (VAE), GAN are able to generate completely original samples.

% Use new MC5 downstream data from MSci to compare GAN-generated and real hits.


% The hit generation rate is X hits/s, whereas a traditional MC simulation starting from the POT collision creates X hits/s. If re-seeding is used to sample particles at the detector region boundary, X hits/s can be produced, so the efficiency gain is about 1e7.
% However the worry is not so much whether or not GANs can yield increase in efficiency (they can), but whether their output respects all the physical properties expected from MC data.

\subsection{\hl{How GAN-generated hits are merged back into a dataset}}



