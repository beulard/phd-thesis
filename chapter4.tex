\chapter{Fake Monte Carlo data generation}
\begin{markdown}
---

- We are coming from the simulation chapter, so:
- Problem description: 
 - We would like to produce larger amounts of simulation data, e.g. for a mock dataset in preparation for data-taking
 - But we have limited capability to produce MC data with simulation. The main bottleneck is in the pion-production section's hadronic interactions. Is there a faster way?


- Creation of dataset from Monte Carlo simulation
  - Hit pattern characterisation
  - Reconstructible vs noise-like classification
- GAN theory [Goodfellow, WGAN, WGAN-GP]
 - Pre-processing
- GAN design, basic architecture, softmax for classes [WGAN-GP]
- GAN extensions, embedding? analogy with text?

---
\end{markdown}


As discussed in the previous chapter, Monte Carlo simulation allows us to draw a realistic picture of what will happen in our experimental setup once it is running. Most importantly, it provides an estimate of the kinds of patterns we can expect to observe in the detector system. 

When performing a full simulation of COMET Phase-I, most of the activity takes place in the initial collision between the proton beam and the graphite target. The many hadronic interactions caused by the high-energy protons represent \SI{99.7}{\percent} of the computational cost of the Monte Carlo simulation.
In contrast to that, because of the large distance between the pion-production section and the detector area, only one muon will come at rest in the stopping target for around every \num{2000} proton-on-target (POT) collisions. Hence, producing Monte Carlo data in the detector system by simulating POT events one by one is a very inefficient process. Not only is most of the computation time spent in determining how secondaries are produced in the proton collision, but statistically, any one collision only has a probability of $\mathcal{O}(\SI{0.1}{\percent})$ to produce meaningful, observable information in the detector system (i.e.\ hits).
% Above number is not strictly true. Muon stopped != hits, beam can produce hits too, and neutrons.
% TODO do a real calculation on MC5
% We can RooTrackerTree->Draw("StdHepN", "StdHepN>0") to get the number of events with a downstream track, but to get the number of events with a hit, it's a bit more involved...
% We need to use the downstream root files
% MC5A01: StdHepN>0 --> 27267993 / 990678400 ~= 2.75% in all RT files
% ---> 2.75% of all POT events lead to >=1 particle entering the detector region
% Repeat on MC5A02 RT files to get stopped muons per POT
% MC5A02: StdHepN>0 && StdHepPdg==13 --> 486051 / 990678399 ~= 0.049%
% ---> 0.049% of all POT events lead to >=1 stopped muon
% /sps/mc5a02/count_events_with_cdc_hits.C --> 5939 / 4980700 ~= 0.12% in first root file
% /sps/mc5a02/count_events_with_cdc_hits.C --> X / 990678399 in all dataset
    % Watch job 20862516 output for answer
% ---> 0.1% of all POT events lead to hit in the CDC

On average, simulating a single POT event in the upstream region requires \SI{2.73}{\second}. A single proton bunch contains \num{16000000} protons, which if simulated linearly would take 500 days of computation. During the data-taking period for COMET Phase-I, $2\times10^{12}$ bunches are expected to hit the target, which would make simulating a similar amount of data impossible on the infrastructure used to produce MC5.

% Why would we want to do that though? Does anyone else do it? Asked Joe
 % Joe says: they only really care about signal MC, MC of just background seems pointless to them...
% Before talking about why it's impossible, we need to bring up the fact that we 
% WANT to get that much data. We need to motivate this...
% Mock-data
% In COMET, one event could make the difference between absence of signal and CLFV discovery.
% Because of the beam transport, it is hard to estimate the kinds background that can enter the detector system.
% -> MC is heavily used to constrain or estimate the rates of various background processes

In order to alleviate this issue, multiple solutions can be considered. One that is currently used in \texttt{SimG4} is to split the simulation into spatial regions, as was described in section~\ref{sec:mc5}. Particles that reach the boundary of the first region are saved and then propagated inside the second region. 
% Reseeding


% Resampling
At the boundary between the regions, we can draw histograms from the position and momentum of particles crossing and later sample from that distribution to generate events in the second region.

Another solution, which we have not yet exploited in COMET, could be to aggressively cut away particles produced in the hadronic interaction which have a small probability of resulting in observable detector hits. To select particles, several options are available such as cuts on position, momentum and particle type, or one could for instance train a more intelligent classifier out of those features.

Lastly, and as is the topic of this chapter, one could partially replace the Monte Carlo simulation by finding % wrong word
a generator which can replicate the patterns of simulated particles in the detectors.

In this chapter, I will discuss my implementation of a Generative Adversarial Networks-based solution to the problem of fast realistic hit generation in the CDC. This restricted application ultimately strives toward a CyDet-wide event sampling algorithm whose lower computational cost compared to traditional Monte Carlo simulation would enable the production of larger datasets, on a scale approaching that of the measurement to be performed.

\section{Procedure}
% Describe the procedure to go from MC data to fake datasets, can use some math abstraction for the GAN, indicating that it could be something else entirely, e.g. G(z) = fake hits, F(x_i, p_i, PID_i) = real hits, distribution(G(z)) ~ distribution(F(.))
Not all MC data can be replaced by an alternative algorithm. Monte Carlo simulation has the significant advantage of providing full truth information about every hit in the detector system, including the type and whereabouts of the particles that produced them.
Truth information is typically useful when a series of hits is susceptible to be reconstructed into a track, whose true provenance will be compared to the guess from the reconstruction algorithm. % Think about this. Is this true? What's the point of reconstruction? Should we consider a hypothetical scenario where we have a dataset with an unknown number of signal tracks and we ask someone to find the needles in the haystack?
% Then why isn't our classification simply signal tracks versus all other tracks?

% Characterisation of tracks in the CDC: high-momentum vs low-momentum
Thankfully, not all hits in the detector require associated truth information, and in fact the majority of hits in the CDC appear like background noise whose origin is irrelevant to any reconstruction procedure. % The question is: DEFINE EXACTLY WHAT KIND OF DATA NEEDS TRUTH
% Illustrate with figure
To illustrate this idea, Fig.~\ref{fig:noise_vs_recon} showcases hits produced by two types of particles.

% So: We've explained why we need to split the data and that we plan on using a hit sampling algorithm to generate one side





\section{Generative Adversarial Networks} % Sampling method
Generative Adversarial Networks (GAN) are a class of data-driven sampling algorithms through which the underlying distribution of a dataset can be modelled and sampled from to obtain new, original samples. % "sample" count: 3 ...
Originally proposed in~\cite{goodfellow_generative_2014}, the method consists in simultaneously training two models in an adversarial process [...].

% Training framework for original GAN: minmax formulation etc

% WGAN, WGAN with Gradient Penalty

% Examples of usage in HEP (see zotero)

% How do we plan on using this to sample CDC hits? Big brain time






