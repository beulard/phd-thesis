\chapter{Discussion}
\label{ch:discussion}

This thesis presents two main original contributions: we designed a fake Monte
Carlo data generator for the CDC using machine learning, and we estimated the rate of
atmospheric muon backgrounds in the COMET Phase-I $\,\mu$--$e$ conversion search
using a backward MC simulation technique. In this chapter, we discuss our main
findings, the limitations we encountered and ideas toward future studies on
either topic.

\section{Data augmentation with machine learning}

\subsection{Summary}
The CDC GAN, discussed in Chapter~\ref{ch:gan}, provides a way to produce
synthetic Monte Carlo data in the Cylindrical Drift Chamber. The design of this
generative algorithm is based on Generative Adversarial Networks, a technique
which is more and more widely used in HEP and in other domains to produce fake
samples by learning from an existing dataset of real samples. The CDC GAN is
trained on MC-simulated hits using the WGAN-GP algorithm. Its discriminator and
generator are convolutional neural networks that can process ordered sequences
of hits, which allows the GAN to learn both from per-hit features, and from the
relations between multiple consecutive hits. Once trained, the GAN is evaluated
in multiple ways. Firstly, the distributions of the synthetic data are compared
to those of the training dataset. In addition, we investigated quality metrics,
which typically involve an external performance criterion implemented by a third
neural network trained independently. These evaluation methods allow us to
determine which aspects of the training data are adequately modelled by the GAN,
and where there are discrepancies. This then helps to guide the design of the
model to increase the quality of generated samples.

% Interpretation
After training, the CDC GAN provides a way to produce original hit data at least
six orders of magnitude more quickly than by means of traditional MC simulation,
using GPUs. We are therefore able to generate large samples of background events
very efficiently. This can be useful, for instance, in studies which require a
pure signal sample to be overlaid onto an ambient background, e.g.\ to optimise
a hit filtering algorithm. Alternatively, GAN-generated data can serve to
compose a mock dataset, combining signal events with all sources of backgrounds
into one large-scale simulation sample used to test the readiness of our
data-processing chain prior to the data acquisition period.




The CDC GAN design and architecture are generalisable to other detector systems
that have discrete elements. For instance, the CTH is somewhat analogous to the
CDC if we treat the scintillation counters like the cells of the CDC. One
difference is that distance of closest approach (\texttt{doca}) has meaning for
CDC hits but not for CTH hits. Although it is not discussed in detail in this
thesis, a CTH GAN prototype was developed based on this analogy, with the
\texttt{wire} feature acting as the counter index, and the
\texttt{doca} feature removed. Based on early evaluations, we observed similar
performance and sample quality as exhibited by the CDC GAN.




% Limitations
\subsection{Limitations and recommendations}
\subsubsection{Training data preselection}
The CDC GAN is only trained on a restricted part of the CDC hit data.
Reconstructible tracks, defined by their momentum $p>\SI{50}{\MeV/\clight}$, are
removed from the training dataset in order to prevent the model from generating
patterns that are too similar to the conversion signal without the proper
associated truth information. Unlike high-momentum tracks whose source
(conversion, DIO, RMC, etc.) matters in studies of the experimental outcomes,
the origin of low-momentum tracks is typically irrelevant, hence those hits can
be grouped into one large ``noise'' category. The GAN then learns only from hits
in that category, and all GAN-generated data can safely be labelled as noise as
well. However, training on a subset of hit data prevents us from using the GAN
to generate an entire background dataset: the GAN must be complemented by real
MC hits of reconstructible tracks, which is computationally expensive to
produce. 

Training a conditional GAN~\cite{Mirza2014ConditionalGA} with the ability to
generate labelled hits was considered. However, the concern that the model would
be unable to grasp the physical laws at play in the hit patterns of
high-momentum particles was too great, hence this solution was discarded. In a
future study, a physics-informed conditional GAN, where the dynamics of charged
particle transport in a magnetic field are encoded into the discriminator in
order to regulate the generator, could be considered to build a universal
background hit generator.

\subsubsection{Feature representation}
One of the difficulties in designing the GAN was in determining the best way to
handle the discrete feature, wire index. Multiple methods were considered, such
as using actual or transformed wire positions as a real-valued feature, or using
a trainable embedding matrix to make the GAN learn its preferred representation
of the geometry. Eventually, we chose to represent the wire index as a one-hot
encoded vector and use a matrix of wire positions to assist the model in
locating wires, since that yielded the most faithful results. However, this
aspect of the GAN design appears to be one of the major quality-limiting factors
because of the large dimensionality of this one-hot vector compared to the other
features. 

A better design would be achieved by representing the wire index as a
low-dimensional quantity while informing both networks of the location of hits,
however a way to fulfil both requirements at once was not found in our
investigations.

\subsubsection{Hit sequence arrangement}

In order to train the GAN as efficiently as possible, we arrange hits from
multiple particles together into fixed-length sequences that can be processed
quickly convolutional layers. This arrangement is not physical: unrelated hits
from different particles become neighbours in the sequence. When processed by a
convolution kernel, consecutive hits will be treated as if there was some
relationship between them (as for neighbour pixels on an image), but this is
sometimes false. This leads the GAN to learn patterns that are artificial and
only the product of our arrangement. 

Keeping hits from different tracks separate would most likely yield a better
model of the underlying tracks. However, it would also prevent the networks from
being built with convolutional layers. Techniques developed and used in natural
language processing, such as transformers~\cite{10.5555/3295222.3295349}, could
be applied here to build a model that can process variable-length hit sequences
(in analogy with sentences) while remaining efficient to train.


\subsubsection{Quality evaluation}

Evaluating samples generated by the CDC GAN is inherently difficult. Unlike
visual or audio data, one cannot precisely estimate the sample quality
perceptually. Hence, in Section~\ref{sec:gan_eval}, we compare feature
distributions and use e.g.\ KL divergence and more complex metrics to define a
quantitative measure of sample quality. Although these methods help in refining
the GAN architecture and in tuning training parameters, no metric can affirm
that the generated data is physical enough to be used to study the experiment
and its outcomes. This is in part due to the fact that the GAN generates
unlabelled hits from the broad category of ``noise'', hence one can only hope to
determine if generated hits belong in that category, or not.

One option could be to condition the GAN~\cite{Mirza2014ConditionalGA} on
particle type and momentum. If the model is able to generate a sequence of hits
based on a prompt, such as ``electron with $p=\SI{15}{\MeV/\clight}$'', then it
becomes easier to verify whether the hit sequence is faithful to the prompt by
comparing to real samples of a similar description \emph{and} by checking that
the physical rules are respected (e.g.\ track curvature, total energy deposit,
etc.). Of course, conditioning comes at the cost of a more complex network
design and training procedure.



\section{Atmospheric muon backgrounds}

\subsection{Summary}
In Chapter~\ref{ch:cosmics}, we presented the backward Monte Carlo simulation
principle, method, validations, and results when applied to the COMET Phase-I
setup. Backward MC consists in the time-reversed transport of particles from a
volume of interest to a source where the flux is known. Since events are
generated arbitrarily near the volume of interest, this provides a way to focus
computational resources on important events. In this study, we used backward MC
transport of atmospheric muons in order to determine the rate of cosmic
ray-induced backgrounds in the $\mu$--$e$ conversion search. Events are
generated from the surface of the Cosmic Ray Veto in Phase-I and transported to
a \SI{1600}{\metre}-high atmospheric plane where a pre-computed flux model is
sampled. The flux estimated at the surface of the CRV is then compared with
experimental data from the BESS-TeV and Kiel-DESY spectrometers in order to
validate that the backward sampling yields realistic results. 

Then, in Chapter~\ref{ch:phase-I_study}, we combine the BMC method with a
$\mu$--$e$ conversion sample and a DIO sample in order to determine how each
process contributes in the event counts recorded during the Phase-I data
acquisition run. The analysis of signal events yields our estimate of the single
event sensitivity of the COMET Phase-I experiment. Integrating over the entire
run time of Phase-I, we find that atmospheric muon backgrounds outnumber the
next largest source of background, DIO, by an order of magnitude or more
depending on the exact conditions. The main factor in eliminating atmospheric
backgrounds is the efficiency of the CRV, followed by the CyDet system's ability
to distinguish between electrons and muons. In the absence of a
particle-identifying Cherenkov layer in the CTH, our results summarised in
Table~\ref{tab:bg_summary} suggest that the background count is at least 0.48
over 146 days, versus one signal event assuming $\mathcal{B}_\mathrm{conversion}
= 3.61 \times 10^{-15}$. With the final-stage CTH layout, the background count
falls to 0.08 assuming a \SI{99}{\percent} $\mu^\pm$ identification rate.

% Implications

% Use vs literature? Actually didn't do a lit review here in HEP...
Backward MC simulation is a powerful tool which enabled us to determine that
extremely rare conversion-like events can be produced by atmospheric muons
sneaking into the COMET detector system. These events can be an important
obstacle for the COMET Phase-I conversion search if no additional way to
reject them, such as particle type identification, is put in place. Using a
standard MC simulation where events are generated far from the detector system,
those rare events that get past the CRV and produce signal-like tracks would
most likely not appear at all in the sample. Hence, the backward MC method is
absolutely key in this study and should continue to prove useful in COMET toward
Phase-I and Phase-II. More generally, any experiment searching for rare
processes and which is susceptible to cosmic ray-induced backgrounds will also
benefit from using backward MC to identify the kinds of atmospheric events that
are likely to mimic their signal, and estimate their frequency.

\subsection{Limitations and recommendations}


\subsubsection{Backward transport}
In our study, we used backward MC simulation to transport muons from the surface
of the CRV to the atmospheric plane. The assumption associated with this
procedure is that the muon is able to travel all the way from the atmospheric
source to the surface of the CRV. In reality, this assumption does not
necessarily hold and there is expected to be some fraction of atmospheric muons
which will interact close to the CRV and whose products will induce a background
event. These events do not contribute toward our results. Therefore, we are
underestimating the background rate from cosmic rays, but by how much is
difficult to determine. 

This bias could be addressed by sampling the muons not at the surface of the
CRV, but a little farther away. However, doing so would lead to more events
missing the detector, as is the case in forward MC, and therefore to wasted
computation. Already, in our study, very few sampled events pass the CyDet
selection criteria: around 1 in $10^5$ in the envelope sample, 1 in $10^7$ for
sneaking events. Pushing the sampling surface outward would decrease this ratio
and reduce our statistics further. Nevertheless, it could be done in the future
in order to estimate the extent of the bias.

\subsubsection{Sampling distributions}

Atmospheric muons are sampled around the CRV with arbitrary energy and direction
distributions. Our selection shows that only one event in \num{100000} produces
a fourfold coincidence and a track in the CyDet system. In a future iteration of
this study, one could determine if there is some region of the sampling phase
space which never contributes to the selected events: perhaps very high or very
low energy events, or events which are incident upon the CRV within a specific
range of azimuthal or zenith angle. Eliminating this region of phase space from
the sampling distribution function would increase the fraction of simulated
events which pass the selection, thus increasing the statistics on events of
interest for the same computational cost.


\subsubsection{Event selection}

To identify background candidates, events are selected that have a fourfold
coincidence in the CTH and an associated track in the CDC. When reconstructing a
trajectory, the track fitting algorithm ignores hits produced by other particles
in the CDC during the same event. Instead of using a track finding algorithm, we
use truth information from the MC simulation in order to select hits from the
right particle and provide them to the track fitting function. This has caused
at least one known inaccuracy in our study: events where a high energy muon
produces a shower in the CDC sometimes induce a fourfold coincidence, thus they
pass our selection criteria and may be counted as background events. In
realistic conditions, events that suddenly saturate a large fraction of CDC
channels, such as a high-energy shower, will most likely be discarded because
tracks cannot be reconstructed from the event. Therefore, these should not have
been counted as part of the integrated background, for instance in
Figures~\ref{fig:log_spectrum_nocuts} and~\ref{fig:bg_count_vs_crv_efficiency}.
These high energy muons typically fly straight through the CyDet system and hit
the CRV along the way. Hence, although this issue in our selection is likely to
have caused a slight overestimation of the total background rate, we expect the
sneaking component to be unaffected.

\subsubsection{Reconstruction and detector response}
Finally, a few other aspects of our study are not faithful to the actual conditions of
Phase-I. On the reconstruction side, for the atmospheric muon sample, we use a
helix fit to reconstruct the trajectory of each event where a fourfold
coincidence is produced. During data acquisition, a more complex fitting
algorithm, based on Kalman filtering, will be used. Similarly, for the
conversion and DIO samples, we do not perform reconstruction but naively
smear the true momentum of the track using the design momentum resolution of the
CDC. Because of those simplifications, we expect the resulting momentum spectra
to differ slightly in the real conditions of Phase-I. 

Although ICEDUST has the capability to fully simulate each sub-detector's
response to incoming energy deposits, digitisation and calibration were not
simulated in this study. Instead, we simplified those processes by applying a
hardware efficiency factor to all samples. Similarly, the timing of events is
also disregarded in our simulation. Instead of assigning a realistic time of
production to conversion, DIO and atmospheric events, and then selecting events
that occur in the trigger time window, we apply an efficiency factor to each
sample based on their expected time distribution and the start and end time of
the trigger window. 

Hence, our study takes a few shortcuts in the detector response and
reconstruction aspects, which we attempt to correct for using
efficiency factors estimated in prior studies. In future studies, and as the
Phase-I data acquisition period approaches, it will be especially important to 
rigorously simulate these effects to assess the performance of the experiment as
accurately as possible.

% \section{Mock data composition}
% Generative algorithms and backward Monte Carlo could in principle be combined to
% compose mock datasets. Consider a mock data challenge where one signal event is
% inserted into a large background sample, and the goal is to find this single
% event among the noise. 
% The mock dataset should contain a realistic mix of hits from all sources of
% background. 
% % Beam
% Beam-induced hits can be produced with a hybrid approach mixing GAN-generated
% hits with MC-simulated reconstructible tracks
% % Cosmic
% For atmospheric-induced hits, 