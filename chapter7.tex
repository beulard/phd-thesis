\chapter{Discussion}

This thesis presents two main original contributions: we designed a fake Monte
Carlo data generator using machine learning, and we estimated the rate of
atmospheric muon backgrounds in the COMET Phase-I $\mu$--$e$ conversion search
using a backward MC simulation technique. In this chapter, we discuss our main
findings, the limitations we encountered and ideas toward future studies on
either topic.

\section{Data augmentation with machine learning}
% Summary
\subsection{Summary}
The CDC GAN, discussed in Chapter~\ref{ch:gan}, provides a way to produce
synthetic Monte Carlo data in the Cylindrical Drift Chamber. The design of this
pair of neural networks is based on Generative Adversarial Networks, a technique
which is more and more widely used in HEP and in other domains to produce fake
samples by learning from an existing dataset of real samples. The CDC GAN is
trained on MC-simulated hits using the WGAN-GP algorithm. Its discriminator and
generator are convolutional neural networks that can process ordered sequences
of hits, which allows the GAN to learn both from per-hit features and from the
relations between multiple consecutive hits. Once trained, the GAN is evaluated
in multiple ways. Firstly, the distributions of the synthetic data are compared
to those of the training dataset. In addition, we investigated quality metrics,
which typically involve an external performance criterion implemented by a third
neural network trained independently. These evaluation methods allow us to
determine which aspects of the training data are adequately modelled by the GAN,
and where generated data deviates from real data. This then helps to guide the
design of the model to increase the quality of generated samples.

% Interpretation
%\subsection{Interpretation}
After training, the CDC GAN provides a way to produce original hit data at least
six orders of magnitude more quickly than by means of traditional MC simulation,
using GPUs. We are therefore able to generate large samples of background events
very efficiently. This can be useful, for instance, in studies which require a
pure signal sample to be overlaid onto an ambient background. Alternatively,
GAN-generated data can serve as a part of a mock dataset, which combines all
sources of backgrounds into one large-scale sample, of a size comparable to the
amount of data collected during the experiment.

\hl{Kind of begs the question of how we would compose a mock dataset here...}




% HERE: Our design can be generalised (applied?) to other detector systems 
% which have discrete active elements, such as for instance the CTH.
% Also a thought... Could we just add the CTH to our one-hot vector / position
% matrix??? --> Not because of DOCA. Hit descriptions are different.

% --> We also trained a GAN for CTH hits based on this design.
The CDC GAN design and architecture are generalisable to other detector systems
which have discrete elements analogous to the CDC wires. For instance, in the
case of the CTH, wires are replaced by scintillation counters and the distance
of closest approach (\texttt{doca}) feature is removed since scintillator hits
can be represented by only a timestamp and energy deposit. Although it is not
discussed in detail in this thesis, a CTH GAN was prototyped, based on the CDC
GAN design. Based on early evaluations, the quality of generated samples seemed
adequate and at least as faithful as the data produced by the CDC GAN.
% Don't know if we include this actually....



% Limitations
\subsection{Limitations and recommendations}
The CDC GAN is only trained on a selected part of the CDC hit data.
Reconstructible tracks, defined by their momentum $p>\SI{50}{\MeV/\clight}$, are
removed from the training dataset of the GAN in order to prevent it from
generating signal-like events. This is done because the GAN generates unlabelled
hits, and does not provide truth information such as parent particle and
momentum, which is required to perform, for instance, background contamination
studies.
Only hits from particles that have a momentum lower than \SI{50}{\MeV/\clight}
are learned from. Although this is preferable to avoid biasing samples, it
prevents us from using the GAN to generate an entire mock dataset: the GAN must
be complemented by real MC hits of reconstructible tracks. In addition, the CDC
GAN fails to take into account timing: it is trained on all non-reconstructible
hit data from a beam MC simulation, which is mostly composed of hits from the
prompt products of the collision. Therefore, it would not be fit to generate a
sample of hits in the trigger time window, and another model should preferably
be trained on this subset of hits.

% Maybe talk about:
One of the difficulties in designing the GAN was in determining the best way to
handle the discrete feature, wire index. Multiple methods were considered, such
as using the actual or transformed wire positions, or using a trainable
embedding matrix to make the GAN learn its preferred representation of the
geometry. Eventually, we chose to represent the wire index as a one-hot encoded
vector and use a matrix of wire positions to assist the model in locating wires,
since that yielded the most faithful results.
% (this is a bit out of place)

Evaluating samples generated by the GAN is inherently difficult. Unlike visual
or audio data, one cannot precisely estimate the sample quality perceptually.
Hence, in Section~\ref{sec:gan_eval} we compare feature distributions and use
e.g.\ KL divergence and more complex metrics to define a quantitative measure of
quality. 
% Here it seems like we're changing the subject a little to quickly
% What's the 'however' for
The training data is initially produced by Monte Carlo simulation of
particles depositing energy in the CDC. Therefore, the rules that govern the
production of hits are rigid physical laws which are difficult to teach to a GAN
model implicitly, through the standard gradient descent algorithm. In fact, the
way in which we arrange hit sequences in the training dataset in order to
optimise the training process already goes against their natural structure. Hits
from a Geant4 simulation are produced by individual particles, so arranging hits
into contiguous, fixed-length arrays is enforcing a particular non-physical
representation onto the GAN model. A better model would therefore likely be
designed by keeping hits from different tracks separate. However, this would
also prevent the network from using the efficient convolution layers that make
up the CDC GAN, since those are specifically designed to act on fixed-length
sequences. 




\section{Atmospheric muon backgrounds}

\subsection{Summary}
In Chapter~\ref{ch:cosmics}, we presented the backward Monte Carlo simulation
principle, method, validations, and results when applied to the COMET Phase-I
setup. Backward MC consists in the time-reversed transport of particles from a
volume of interest to a source where the flux is known. Since events are
generated arbitrarily near the volume of interest, this provides a way to focus
computational resources on important events. In this study, we use backward MC
transport of atmospheric muons in order to determine the rate of cosmic
ray-induced backgrounds in the $\mu$--$e$ conversion search. Events are
generated from the surface of the Cosmic Ray Veto in Phase-I and transported to
a \SI{1600}{\metre}-high atmospheric plane where a pre-computed flux model is
sampled. The flux estimated at the surface of the CRV is then compared with
experimental data from the BESS-TeV and Kiel-DESY spectrometers in order to
validate that the backward sampling yields realistic results. 

Then, in Chapter~\ref{ch:phase-I_study}, we combine the BMC method with a
$\mu$--$e$ conversion sample and a DIO sample in order to determine how each
process contributes in the event counts recorded during the Phase-I data
acquisition run. The analysis of signal events yields our estimate of the single
event sensitivity of the COMET Phase-I experiment. Integrating over the entire
data acquisition run time of Phase-I, we find that atmospheric muon backgrounds
outnumber the next largest source of background, DIO, by an order of magnitude
or more depending on the exact conditions. The main factor in eliminating
atmospheric backgrounds is the efficiency of the CRV, followed by the CyDet
system's ability to distinguish between electrons and muons. In the absence of a
particle-identifying Cherenkov layer in the CTH, our results suggest that the
background count is at least 0.48 over 146 days, versus one signal event
assuming $\mathcal{B}_\mathrm{conversion} = 3.61 \times 10^{-15}$. With the
final-stage CTH layout, the background count falls to 0.08 assuming a
\SI{99}{\percent} $\mu^\pm$ identification rate.

% Implications

% Use vs literature? Actually didn't do a lit review here in HEP...
Backward MC simulation is a powerful tool which enabled us to determine that
extremely rare conversion-like events can be produced by atmospheric muons
sneaking into the COMET detector system. These events can be an important
obstacle for the COMET Phase-I conversion search if no additional way to
identify them, such as particle type identification, is put in place. Using a
standard MC simulation where events are generated far from the detector system,
those rare events that get past the CRV and produce signal-like tracks would
most likely not appear at all in the sample. Hence, the backward MC method is
absolutely key in this study and should continue to prove useful in COMET toward
Phase-I and Phase-II. More generally, any experiment searching for rare
processes and which is susceptible to cosmic ray-induced backgrounds will also
benefit from using backward MC to identify the kinds of atmospheric events that
are likely to mimic their signal, and estimate their frequency.

\subsection{Limitations and recommendations}

\hl{Perhaps we should have subsubsections here, it's very long.}

% Can't use it for other particles, e.g. electrons that could also (?) sneak
% through the openings, unless we place the sampling surface farther away from
% the CRV. As it stands in our study, we only consider backgrounds from muons
% that make it all the way from the sky to the CRV surface.
In our study, we used backward MC simulation to transport muons from the surface
of the CRV to the atmospheric plane. The assumption associated with this
procedure is that the muon would not first interact outside the CRV volume and
produce some secondary particle which would then enter the detector. In other
words, we only consider events where the muon is able to travel all the way
from the atmospheric source to the surface of the CRV.
In reality, this assumption does not necessarily hold and there is expected to
be some fraction of atmospheric muons which will first interact close to, but
not inside, the CRV and whose products will induce a background event.
Therefore, we are underestimating the background rate from cosmic rays, but by
how much is difficult to determine. 

This bias could be addressed by sampling the muons not at the surface of the
CRV, but a little farther away. However, doing so leads to more events missing
the detector, as is the case in forward MC, and therefore wasted computation.
Already, in our study, very few sampled events pass the CyDet selection criteria
(around 1 in $10^6$ in the envelope sample, 1 in $10^8$ for sneaking events).
Expanding the sampling surface would make these ratios smaller and reduce our
statistics even further. % Conclude about this bias?

In future studies, the sampling strategy should be increasingly refined to cause
more of the sampled events to produce signal-like

% Also: we don't check that the CDC isn't flooded by high energy muons, which is
% the case in some events.
In our selection, we pick events that have a fourfold coincidence in the CTH and
an associated track in the CDC. When reconstructing a trajectory, the track
fitting algorithm ignores hits produced by other particles in the CDC during the
same event. Instead of using a track finding algorithm, we use truth information
from the MC simulation in order to select hits from the right particle and
provide them to the track fitting function. This has caused at least one known
inaccuracy in our study: events where a high energy muon produces a shower in
the CDC sometimes induce a fourfold coincidence, thus they pass our selection
criteria and may be counted as background events. In realistic conditions,
events that suddenly flood a large fraction of CDC channels will most likely be
easily identified as an atmospheric muon shower and rejected. Therefore, they
should not have been counted as part of the integrated background, for instance
in Figures~\ref{fig:log_spectrum_nocuts}
and~\ref{fig:bg_count_vs_crv_efficiency}. These high energy events typically fly
straight through the CyDet system and hit the CRV along the way, hence we
expect the component of the background count associated with sneaking events
to be unchanged by this inaccuracy in our selection algorithm.

% Also: helix fit isn't realistic.
% And neither is momentum smearing for the signal and bg samples.
% And we should simulate detector response.
% And timing.
Finally, a few other aspects of our study are not faithful to the actual conditions of
Phase-I. On the reconstruction side, for the atmospheric muon sample, we use a
helix fit to reconstruct the trajectory of each event where a fourfold
coincidence is produced. During data acquisition, a more complex fitting
algorithm, based on Kalman filtering, will be used. Similarly, for the
conversion and DIO samples, we do not perform reconstruction at all and naively
smear the true momentum of the track using the design momentum resolution of the
CDC. Because of those simplifications, we expect the resulting momentum spectra
to differ slightly in the real conditions of Phase-I. 

Although ICEDUST has the capability to fully simulate each sub-detector's
response to incoming energy deposits, digitisation and calibration were not
simulated in this study. Instead, we simplified those processes by applying a
hardware efficiency factor to all samples. 
Similarly, the timing of events is also disregarded in our simulation. Instead
of assigning a realistic time of production to conversion, DIO and atmospheric
events, and then selecting events that occur in the trigger time window, we
ignore timing altogether and apply an efficiency factor to each sample. 

Hence, our study takes a few shortcuts in the detector response and
reconstruction aspects of Phase-I, which we attempted to correct for using
efficiency factors estimated in prior studies. In future studies, and as the
Phase-I data acquisition period approaches, it will be especially useful to be
more thorough and estimate these efficiency factors through proper simulation of
the associated processes.


% Recommendations

\hl{ OK so I wrote about pretty much all I wanted to, or close. Nice. Now to
    format everything, and review.}


\subsection{Mock daTa?}