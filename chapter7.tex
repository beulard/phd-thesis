\chapter{Discussion}

This thesis presents two main original contributions: we designed a fake Monte
Carlo data generator using machine learning, and we estimated the rate of
atmospheric muon backgrounds in the COMET Phase-I $\mu$--$e$ conversion search
using a backward MC simulation technique. In this chapter, we discuss the main
findings from both studies, the limitations we encountered and ideas toward
future studies on either topic.

\section{Data augmentation with machine learning}
% Summary
\subsection{Summary}
The CDC GAN, discussed in Chapter~\ref{ch:gan}, provides a way to produce
synthetic Monte Carlo data in the Cylindrical Drift Chamber. The design of this
pair of neural networks is based on Generative Adversarial Networks, a technique
widely used in HEP and in other domains to produce fake samples by learning from
a dataset of real samples. The CDC GAN is trained on a dataset of MC-simulated
hits, using the WGAN-GP algorithm. Its discriminator and generator are
convolutional neural networks that can process ordered sequences of hits, which
allows the GAN to learn both from per-hit features and from the relations
between multiple consecutive hits. 
Once trained, the GAN is evaluated in multiple ways. First, the distributions of
the synthetic data are compared to those of the training dataset. Then, quality
metrics are investigated, which typically involve an external performance criterion
implemented by a third neural network trained independently. These evaluation
methods allow us to determine which aspects of the training data are adequately
modelled by the GAN, and where generated data deviates from real data. This then
helps to guide the design of the machine learning model to increase the quality
of generated samples.

% Interpretation
%\subsection{Interpretation}
After training, the CDC GAN provides a way to produce original hit data $10^7$
times more quickly than by means of traditional MC simulation, using GPUs. We
are therefore able to generate large samples of background events very efficiently.
This can be useful, for instance, in studies which require a pure signal sample
to be overlaid onto an ambient background. Alternatively, GAN-generated data can
serve as a part of a mock dataset, which combines all sources of backgrounds
into one large-scale sample, of a size comparable to the amount of data
collected during the experiment.


% Implications (relevance vs literature)

% Maybe talk about:
One of the difficulties in designing the GAN was in determining the best way
to handle the discrete feature, wire index. For the CDC GAN, we chose to
represent the wire index as a one-hot encoded vector. However, other methods
were investigated, such as using the actual or transformed wire positions, or
using a trainable embedding matrix to make the GAN learn its preferred
representation of the geometry.



% Limitations
\subsection{Limitations and recommendations}
% + CDC GAN can only produce a part of the whole: noise-like hit data only.
The CDC GAN is only trained on a selected part of the CDC hit data.
Reconstructible tracks, defined by their momentum $p>\SI{50}{\MeV/\clight}$, are
removed from the training dataset of the GAN in order to prevent it from
generating signal-like events. This is done because the GAN generates unlabelled
hits, and does not provide truth information such as parent particle and
momentum, which is required to perform, for instance, background contamination
studies.
Only hits from particles that have a momentum lower than \SI{50}{\MeV/\clight}
are learned from. Although this is preferable to avoid biasing samples, it
prevents us from using the GAN to generate an entire mock dataset: the GAN must
be complemented by real MC hits of reconstructible tracks.
In addition, the CDC GAN fails to take into account timing: it is trained on all
hit data from a beam MC simulation, which is mostly made up of hits from the
prompt products of the collision. Therefore, it would not be fit to generate a
sample of hits in the trigger time window, and another model trained on this
subset of hits would need to be trained.

Evaluating samples generated by the GAN is inherently difficult. Unlike visual
or audio data, one cannot precisely estimate the sample quality perceptually.
Hence, we compare feature distributions and use e.g.\ KL divergence and more
complex metrics to define a quantitative measure of quality. However, the
training data is initially produced by Monte Carlo simulation of particles
depositing energy in the CDC. Therefore, the rules that govern the production of
hits are rigid physical laws which are difficult to teach to a GAN model
implicitly, through the standard gradient descent algorithm. In fact, the way in
which we arrange hit sequences in the training dataset in order to optimise the
training process already goes against their natural structure. Hits from a
Geant4 simulation are produced by individual particles, so arranging hits into
contiguous, fixed-length arrays is enforcing a particular non-physical
representation onto the GAN model. A better model would therefore likely be
designed by keeping hits from different tracks separate. However, this would
also prevent the network from using the efficient convolution layers that make
up the CDC GAN, since those are specifically designed to act on fixed-length
sequences. 

% + Evaluating the quality of generated samples, in absolute terms or in physical
%   terms is difficult.


% Recommendations
% + Design algorithm not for efficiency, but for faithfulness.

%\subsection{Mock data production}

\section{Atmospheric muon backgrounds}

\subsection{Summary}
In Chapter~\ref{ch:cosmics}, we presented the backward Monte Carlo simulation
principle, method, validations, and results when applied to the COMET Phase-I
setup. Backward MC consists in the time-reversed transport of particles from a
volume of interest to a source where the flux is known. Since events are
generated arbitrarily near the volume of interest, backward MC provides a way to
focus computational resources on important events. We use backward MC transport
of atmospheric muons from the surface of the Cosmic Ray Veto in Phase-I, to a
\SI{1600}{\metre}-high atmospheric plane where their flux has been pre-computed.
The flux computed at the surface of the CRV is compared with experimental data
from the BESS-TeV and Kiel-DESY spectrometers in order to validate that the
backward sampling yields realistic results. We analyse the sample to estimate
the rate of background events caused by atmospheric muons in the $\mu$--$e$
conversion search.

Then, in Chapter~\ref{ch:phase-I_study}, we combine the BMC method with a
$\mu$--$e$ conversion sample and a DIO sample in order to determine how each
process contributes in the event counts recorded during the Phase-I data
acquisition run. The analysis of signal events yields our estimate of the single
event sensitivity of the COMET Phase-I experiment. Integrating over the entire
data acquisition run time of Phase-I, we find that atmospheric muon backgrounds
outnumber the other main source of background, DIO, by an order of magnitude or
more depending on the exact conditions. The main factor in eliminating
atmospheric backgrounds is the efficiency of the CRV, followed by the CyDet
system's ability to distinguish between electrons and muons. In the absence of a
Cherenkov layer in the CTH, our results suggest that the background count is at
least 0.48 over 146 days, versus one signal event assuming
$\mathcal{B}_\mathrm{conversion} = 3.61 \times 10^{-15}$. With the final-stage
CTH layout, the background count falls to 0.08 assuming a \SI{99}{\percent}
$\mu^\pm$ identification rate.

% Implications

% Use vs literature? Actually didn't do a lit review here in HEP...
Backward MC simulation is a powerful tool which enabled us to determine that
extremely rare conversion-like events can be produced by atmospheric muons
sneaking into the COMET detector system. These events can be an important
obstacle for the COMET Phase-I conversion search if no other way to identify
them, such as particle type identification, is put in place. Using a standard MC
simulation where events are generated far from the detector system, those rare
events that get past the CRV and produce signal-like tracks would most likely
not appear at all in the sample. Hence, the backward MC method is absolutely key
in this study and should continue to prove useful in COMET toward Phase-I and
Phase-II. Any experiment searching for rare processes and which is susceptible
to cosmic ray-induced backgrounds will also benefit from using this technique to
determine the kinds of events that are most likely to mimic their signal.

\subsection{Limitations and recommendations}
% Can't use it for other particles, e.g. electrons that could also (?) sneak
% through the openings, unless we place the sampling surface farther away from
% the CRV. As it stands in our study, we only consider backgrounds from muons
% that make it all the way from the sky to the CRV surface.
In our study, we used backward MC simulation to transport muons from the surface
of the CRV to the atmospheric plane. The assumption associated with this
procedure is that the muon will not interact outside the CRV volume and produce
some secondary particle which will then enter the detector. In reality, this
assumption does not hold and there is bound to be some fraction of atmospheric
muons which will first interact close to, but not inside, the CRV and whose
products will induce a background event. Therefore, we are underestimating the
background rate from cosmic rays, but by how much is impossible to say. 

This bias could be addressed by sampling the muons not at the surface of the
CRV, but a little farther away. However, doing so leads to more events missing
the detector, as is the case in forward MC, and therefore wasted computations.
Already, in our study, very few sampled events pass the CyDet selection criteria
(around 1 in $10^6$ in the envelope sample, 1 in $10^8$ for sneaking events).
Expanding the sampling surface would make this ratio smaller and reduce our
statistics even further.

In future studies, the sampling strategy should be increasingly refined to cause
more of the sampled events to produce signal-like

% Also: we don't check that the CDC isn't flooded by high energy muons, which is
% the case in some events.

% Also: helix fit isn't realistic.
% And neither is momentum smearing for the signal and bg samples.
% And we should simulate detector response.
% And timing.


% Recommendations
